{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64a2261",
   "metadata": {
    "papermill": {
     "duration": 0.00949,
     "end_time": "2023-05-04T08:48:09.419808",
     "exception": false,
     "start_time": "2023-05-04T08:48:09.410318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3585843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:09.439420Z",
     "iopub.status.busy": "2023-05-04T08:48:09.438663Z",
     "iopub.status.idle": "2023-05-04T08:48:09.449178Z",
     "shell.execute_reply": "2023-05-04T08:48:09.448310Z"
    },
    "papermill": {
     "duration": 0.022643,
     "end_time": "2023-05-04T08:48:09.451379",
     "exception": false,
     "start_time": "2023-05-04T08:48:09.428736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85506565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:09.468667Z",
     "iopub.status.busy": "2023-05-04T08:48:09.468419Z",
     "iopub.status.idle": "2023-05-04T08:48:15.300658Z",
     "shell.execute_reply": "2023-05-04T08:48:15.299727Z"
    },
    "papermill": {
     "duration": 5.843735,
     "end_time": "2023-05-04T08:48:15.303272",
     "exception": false,
     "start_time": "2023-05-04T08:48:09.459537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b61194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:15.320765Z",
     "iopub.status.busy": "2023-05-04T08:48:15.320292Z",
     "iopub.status.idle": "2023-05-04T08:48:15.324556Z",
     "shell.execute_reply": "2023-05-04T08:48:15.323598Z"
    },
    "papermill": {
     "duration": 0.015321,
     "end_time": "2023-05-04T08:48:15.326725",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.311404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c750d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:15.343743Z",
     "iopub.status.busy": "2023-05-04T08:48:15.343479Z",
     "iopub.status.idle": "2023-05-04T08:48:15.348469Z",
     "shell.execute_reply": "2023-05-04T08:48:15.347540Z"
    },
    "papermill": {
     "duration": 0.0157,
     "end_time": "2023-05-04T08:48:15.350310",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.334610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98c8f9",
   "metadata": {
    "papermill": {
     "duration": 0.007466,
     "end_time": "2023-05-04T08:48:15.365906",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.358440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set the device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8acd12",
   "metadata": {
    "papermill": {
     "duration": 0.007825,
     "end_time": "2023-05-04T08:48:15.381524",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.373699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> This is the most important step!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f28e9e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:15.398538Z",
     "iopub.status.busy": "2023-05-04T08:48:15.398230Z",
     "iopub.status.idle": "2023-05-04T08:48:15.492669Z",
     "shell.execute_reply": "2023-05-04T08:48:15.491745Z"
    },
    "papermill": {
     "duration": 0.105264,
     "end_time": "2023-05-04T08:48:15.494664",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.389400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e2774",
   "metadata": {
    "papermill": {
     "duration": 0.007841,
     "end_time": "2023-05-04T08:48:15.511117",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.503276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Load the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fa86ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:15.527806Z",
     "iopub.status.busy": "2023-05-04T08:48:15.527471Z",
     "iopub.status.idle": "2023-05-04T08:48:15.531978Z",
     "shell.execute_reply": "2023-05-04T08:48:15.531078Z"
    },
    "papermill": {
     "duration": 0.014783,
     "end_time": "2023-05-04T08:48:15.533758",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.518975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pickle(f_name):\n",
    "    with open(f_name,\"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23ffc83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:15.551017Z",
     "iopub.status.busy": "2023-05-04T08:48:15.550193Z",
     "iopub.status.idle": "2023-05-04T08:48:54.031487Z",
     "shell.execute_reply": "2023-05-04T08:48:54.030402Z"
    },
    "papermill": {
     "duration": 38.492198,
     "end_time": "2023-05-04T08:48:54.033872",
     "exception": false,
     "start_time": "2023-05-04T08:48:15.541674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_text_tensors = load_pickle(\"/kaggle/input/converting-text-to-tensors-ipynb/train_text_tensors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0135206a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:48:54.052722Z",
     "iopub.status.busy": "2023-05-04T08:48:54.050976Z",
     "iopub.status.idle": "2023-05-04T08:49:03.338876Z",
     "shell.execute_reply": "2023-05-04T08:49:03.336600Z"
    },
    "papermill": {
     "duration": 9.301824,
     "end_time": "2023-05-04T08:49:03.343971",
     "exception": false,
     "start_time": "2023-05-04T08:48:54.042147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_text_tensors = load_pickle(\"/kaggle/input/converting-text-to-tensors-ipynb/test_text_tensors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703c2732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.392451Z",
     "iopub.status.busy": "2023-05-04T08:49:03.390825Z",
     "iopub.status.idle": "2023-05-04T08:49:03.421386Z",
     "shell.execute_reply": "2023-05-04T08:49:03.419393Z"
    },
    "papermill": {
     "duration": 0.05913,
     "end_time": "2023-05-04T08:49:03.425651",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.366521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels_tensors = load_pickle(\"/kaggle/input/converting-text-to-tensors-ipynb/train_labels_tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1992c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.475586Z",
     "iopub.status.busy": "2023-05-04T08:49:03.473976Z",
     "iopub.status.idle": "2023-05-04T08:49:03.494984Z",
     "shell.execute_reply": "2023-05-04T08:49:03.493361Z"
    },
    "papermill": {
     "duration": 0.050456,
     "end_time": "2023-05-04T08:49:03.499562",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.449106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels_tensors = load_pickle(\"/kaggle/input/converting-text-to-tensors-ipynb/test_labels_tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7041e7",
   "metadata": {
    "papermill": {
     "duration": 0.007882,
     "end_time": "2023-05-04T08:49:03.523524",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.515642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89a25219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.540053Z",
     "iopub.status.busy": "2023-05-04T08:49:03.539725Z",
     "iopub.status.idle": "2023-05-04T08:49:03.545330Z",
     "shell.execute_reply": "2023-05-04T08:49:03.544360Z"
    },
    "papermill": {
     "duration": 0.016106,
     "end_time": "2023-05-04T08:49:03.547295",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.531189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextLabelDataset(Dataset):\n",
    "    def __init__(self,text_tensor,label_tensor):\n",
    "        \n",
    "        self.text_tensor = text_tensor\n",
    "        self.label_tensor = label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_tensor)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.text_tensor[idx],self.label_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9528e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.563890Z",
     "iopub.status.busy": "2023-05-04T08:49:03.563544Z",
     "iopub.status.idle": "2023-05-04T08:49:03.568835Z",
     "shell.execute_reply": "2023-05-04T08:49:03.568059Z"
    },
    "papermill": {
     "duration": 0.01567,
     "end_time": "2023-05-04T08:49:03.570701",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.555031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = TextLabelDataset(train_text_tensors,train_labels_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e57d37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.587058Z",
     "iopub.status.busy": "2023-05-04T08:49:03.586788Z",
     "iopub.status.idle": "2023-05-04T08:49:03.594641Z",
     "shell.execute_reply": "2023-05-04T08:49:03.593751Z"
    },
    "papermill": {
     "duration": 0.018145,
     "end_time": "2023-05-04T08:49:03.596527",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.578382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gc.collect(generation=2)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_text_tensors\n",
    "del train_labels_tensors\n",
    "gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "709aed98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.613165Z",
     "iopub.status.busy": "2023-05-04T08:49:03.612865Z",
     "iopub.status.idle": "2023-05-04T08:49:03.617203Z",
     "shell.execute_reply": "2023-05-04T08:49:03.616144Z"
    },
    "papermill": {
     "duration": 0.01494,
     "end_time": "2023-05-04T08:49:03.619309",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.604369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = TextLabelDataset(test_text_tensors,test_labels_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5544a40b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.636144Z",
     "iopub.status.busy": "2023-05-04T08:49:03.635894Z",
     "iopub.status.idle": "2023-05-04T08:49:03.782780Z",
     "shell.execute_reply": "2023-05-04T08:49:03.781779Z"
    },
    "papermill": {
     "duration": 0.157389,
     "end_time": "2023-05-04T08:49:03.784691",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.627302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_text_tensors\n",
    "del test_labels_tensors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8973bee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.802375Z",
     "iopub.status.busy": "2023-05-04T08:49:03.802070Z",
     "iopub.status.idle": "2023-05-04T08:49:03.806684Z",
     "shell.execute_reply": "2023-05-04T08:49:03.805739Z"
    },
    "papermill": {
     "duration": 0.015487,
     "end_time": "2023-05-04T08:49:03.808547",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.793060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#batch size of 32 is too large for kaggle mem, reducing it to 4\n",
    "#if you are reducing the size of batch , also reduce the learning rate! because sgd will update after each batch. and since now the batches are small,hence more frequent updates,hence the lr should be less\n",
    "train_loader = DataLoader(train_dataset,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d8c5446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.825923Z",
     "iopub.status.busy": "2023-05-04T08:49:03.825637Z",
     "iopub.status.idle": "2023-05-04T08:49:03.959696Z",
     "shell.execute_reply": "2023-05-04T08:49:03.958750Z"
    },
    "papermill": {
     "duration": 0.144969,
     "end_time": "2023-05-04T08:49:03.961602",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.816633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5e26dca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:03.979392Z",
     "iopub.status.busy": "2023-05-04T08:49:03.979129Z",
     "iopub.status.idle": "2023-05-04T08:49:03.984378Z",
     "shell.execute_reply": "2023-05-04T08:49:03.983587Z"
    },
    "papermill": {
     "duration": 0.016317,
     "end_time": "2023-05-04T08:49:03.986283",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.969966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6553a5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:04.003471Z",
     "iopub.status.busy": "2023-05-04T08:49:04.003212Z",
     "iopub.status.idle": "2023-05-04T08:49:04.134864Z",
     "shell.execute_reply": "2023-05-04T08:49:04.133976Z"
    },
    "papermill": {
     "duration": 0.142671,
     "end_time": "2023-05-04T08:49:04.137160",
     "exception": false,
     "start_time": "2023-05-04T08:49:03.994489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2771f0c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:04.155445Z",
     "iopub.status.busy": "2023-05-04T08:49:04.154864Z",
     "iopub.status.idle": "2023-05-04T08:49:04.258912Z",
     "shell.execute_reply": "2023-05-04T08:49:04.257896Z"
    },
    "papermill": {
     "duration": 0.115854,
     "end_time": "2023-05-04T08:49:04.261442",
     "exception": false,
     "start_time": "2023-05-04T08:49:04.145588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of text tensor torch.Size([4, 16788])\n",
      "Shape of label tensor torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "##having a sneak at one train batch\n",
    "\n",
    "for idx,data in enumerate(train_loader):\n",
    "    text_tensor,label_tensor = data\n",
    "    \n",
    "    print(f\"Shape of text tensor {text_tensor.shape}\")\n",
    "    print(f\"Shape of label tensor {label_tensor.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fb53b",
   "metadata": {
    "papermill": {
     "duration": 0.008191,
     "end_time": "2023-05-04T08:49:04.277915",
     "exception": false,
     "start_time": "2023-05-04T08:49:04.269724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building a Custom Model using Model parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b31ae9e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:04.296045Z",
     "iopub.status.busy": "2023-05-04T08:49:04.295211Z",
     "iopub.status.idle": "2023-05-04T08:49:04.301890Z",
     "shell.execute_reply": "2023-05-04T08:49:04.301078Z"
    },
    "papermill": {
     "duration": 0.017836,
     "end_time": "2023-05-04T08:49:04.303888",
     "exception": false,
     "start_time": "2023-05-04T08:49:04.286052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextClassification(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_dim,hidden_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        self.embed = nn.Embedding(vocab_size,hidden_dim,padding_idx=0)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.linear = nn.Linear(hidden_dim,32)\n",
    "        self.gelu2 = nn.GELU()\n",
    "        self.final = nn.Linear(32,3)#since there are 6 classes in the dataset\n",
    "    \n",
    "    def forward(self,text_tensor):\n",
    "        output = self.embed(text_tensor)\n",
    "        output = self.gelu1(output)\n",
    "        output = self.linear(output)\n",
    "        output = self.gelu2(output)\n",
    "        output = self.final(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a35c5",
   "metadata": {
    "papermill": {
     "duration": 0.008047,
     "end_time": "2023-05-04T08:49:04.319723",
     "exception": false,
     "start_time": "2023-05-04T08:49:04.311676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Okay then! we can proceed with writing the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff861003",
   "metadata": {
    "papermill": {
     "duration": 0.008232,
     "end_time": "2023-05-04T08:49:04.336215",
     "exception": false,
     "start_time": "2023-05-04T08:49:04.327983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### But wait a min!! we still have bunch of things left to do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7b599",
   "metadata": {
    "papermill": {
     "duration": 0.00803,
     "end_time": "2023-05-04T08:49:04.352903",
     "exception": false,
     "start_time": "2023-05-04T08:49:04.344873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set the model to the current device i.e cpu or gpu\n",
    "\n",
    "> note: we have already set the input tensors to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "598c0ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:04.371051Z",
     "iopub.status.busy": "2023-05-04T08:49:04.370242Z",
     "iopub.status.idle": "2023-05-04T08:49:08.896422Z",
     "shell.execute_reply": "2023-05-04T08:49:08.895432Z"
    },
    "papermill": {
     "duration": 4.538224,
     "end_time": "2023-05-04T08:49:08.899298",
     "exception": false,
     "start_time": "2023-05-04T08:49:04.361074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TextClassification(vocab_size = 16788,embed_dim=512,hidden_dim=128)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac9bcb",
   "metadata": {
    "papermill": {
     "duration": 0.009773,
     "end_time": "2023-05-04T08:49:08.920170",
     "exception": false,
     "start_time": "2023-05-04T08:49:08.910397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set the loss function:\n",
    "\n",
    "Since we are dealing with a classification problem here , we will use cross entopy loss function\n",
    "which basically calculates the entropy between the probability distribuition which is outputed by the model and the gold probability distribuition.\n",
    "\n",
    "The more similar the probability distribution the less will the entropy i.e the loss\n",
    "\n",
    "> Foumula for binary classification for a single instance\n",
    "loss = -(p(gold output)*log(p(predicted output)) - (1-p(gold output))*log(1-p(predicted output)))\n",
    "\n",
    "> The above can be be generalized for multiple classes\n",
    "loss = -sum for each class(p(gold output)*log(p(predicted output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61c93651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:08.939705Z",
     "iopub.status.busy": "2023-05-04T08:49:08.939375Z",
     "iopub.status.idle": "2023-05-04T08:49:08.944022Z",
     "shell.execute_reply": "2023-05-04T08:49:08.942905Z"
    },
    "papermill": {
     "duration": 0.016677,
     "end_time": "2023-05-04T08:49:08.946119",
     "exception": false,
     "start_time": "2023-05-04T08:49:08.929442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f492f33",
   "metadata": {
    "papermill": {
     "duration": 0.008576,
     "end_time": "2023-05-04T08:49:08.963427",
     "exception": false,
     "start_time": "2023-05-04T08:49:08.954851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set the optimizer\n",
    "\n",
    "The optimizer decides how the gradient will be updated in backpropagation.Some of the commonly known optimizers are :\n",
    "\n",
    "> 1) SGD : This is the same as mini batch gradient descent .w_new = W_old - grad_of_loss_wrt_w\n",
    "\n",
    "> 2) SGD with momentum : Now SGD can get stuck it local minima,this borrows the intuition of momentum in physics, where the momentum due to previous gradient updates can push the parameter out of its local minima and lead towards global minima.  w_new = W_old + momentum, where momentum = function(grad_wrt_w_old, previous momentum)\n",
    "\n",
    "> 3) Adagrad : Often times, using only momentum might be slow, as the gradients of many paprameters might tend to zero and not contirbute to gradient descent. Here we maintain the sums of squares of gradient and divide the learning rate of each paramater by the square root of the accualted sums of gradients for that particular parameter. Hence unlike momentum , different parameters will have different learning rates , which is better.\n",
    "\n",
    "> 4) RMSProp : Since Adagrad uses squares of gradients, it can accumulate gradients too quickly and thus cause the learning rates to drop too rapidly. RMSProp thus handles the problem of Adagrad.\n",
    "\n",
    "> 5) Adam : Adam basically combines the adavantages of momentum and rmsProp. With momentum it acheives the speed of convergence and with rmsprop every parameter gets to have its own learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71836acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:08.982059Z",
     "iopub.status.busy": "2023-05-04T08:49:08.981292Z",
     "iopub.status.idle": "2023-05-04T08:49:08.986450Z",
     "shell.execute_reply": "2023-05-04T08:49:08.985574Z"
    },
    "papermill": {
     "duration": 0.016532,
     "end_time": "2023-05-04T08:49:08.988419",
     "exception": false,
     "start_time": "2023-05-04T08:49:08.971887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting sgd with momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "336a7fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:09.007814Z",
     "iopub.status.busy": "2023-05-04T08:49:09.007166Z",
     "iopub.status.idle": "2023-05-04T08:49:09.015189Z",
     "shell.execute_reply": "2023-05-04T08:49:09.014396Z"
    },
    "papermill": {
     "duration": 0.019914,
     "end_time": "2023-05-04T08:49:09.016979",
     "exception": false,
     "start_time": "2023-05-04T08:49:08.997065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_for_one_epoch(dataloader,epoch_nos):\n",
    "    '''\n",
    "        One epoch will do forward pass and backward pass for the model for the entire dataset for one iteration.\n",
    "        For convergence, we would have do forward and backward pass multiple times for the dataset\n",
    "\n",
    "    '''\n",
    "    loss_for_the_epoch = 0\n",
    "    \n",
    "    #iterate through the dataset in batches, i.e we will at a time load the chuncks(of size = batch size) of dataset\n",
    "    for idx,data in enumerate(dataloader):\n",
    "        \n",
    "        input_tensors,label_tensors = data\n",
    "        \n",
    "        input_tensors = input_tensors.to(device)\n",
    "        label_tensors = label_tensors.to(device)\n",
    "        \n",
    "        #embedding layer needs the input to be of int\n",
    "        input_tensors = input_tensors.to(torch.int)\n",
    "        \n",
    "        #label tensor required by the loss function is to be long\n",
    "        label_tensors = label_tensors.to(torch.long)\n",
    "        \n",
    "        #refer to the notebook on autograd in the repository and recall how we calculate gradients.\n",
    "        #optimizer will store the .grad value of all the parameters, hence at the start of our iteration, we will set the grads to 0\n",
    "        #this is important, as we do not want to accumulate the gradients for each batch.\n",
    "        #since we are doing sgd, we will update the gradients(via backprop) at the end of each batch and clear the gradients at the begining of each batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #get the ouput of the model by doing forward pass\n",
    "        model_output = model(input_tensors)\n",
    "        \n",
    "        #compute the loss\n",
    "        loss = loss_fn(model_output,label_tensors)\n",
    "        \n",
    "        #now that we have the loss!, we would want to update all the parameters of our model by backprop\n",
    "        #and recall this step from the autograd notebook as well\n",
    "        #hence this step will calculate and update the .grad values of all the parameters in our model\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        #and now that we have the gradient values of all our parameter, we would do gradient descent for the parameters. i.e upate the parameter values\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_for_the_epoch += loss.item() \n",
    "        \n",
    "        #print the loss for every 500th batch\n",
    "        if idx % 1000 == 999:\n",
    "            curr_loss = loss_for_the_epoch/1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(idx + 1, curr_loss))\n",
    "    \n",
    "    \n",
    "    return loss_for_the_epoch/len(dataloader)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7e2f6",
   "metadata": {
    "papermill": {
     "duration": 0.007808,
     "end_time": "2023-05-04T08:49:09.032914",
     "exception": false,
     "start_time": "2023-05-04T08:49:09.025106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Code for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6426a41a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:09.051086Z",
     "iopub.status.busy": "2023-05-04T08:49:09.050195Z",
     "iopub.status.idle": "2023-05-04T08:49:09.056377Z",
     "shell.execute_reply": "2023-05-04T08:49:09.055572Z"
    },
    "papermill": {
     "duration": 0.017089,
     "end_time": "2023-05-04T08:49:09.058107",
     "exception": false,
     "start_time": "2023-05-04T08:49:09.041018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(dataloader):\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    for idx,data in enumerate(dataloader):\n",
    "        input_tensor,label = data\n",
    "        \n",
    "        input_tensor = input_tensor.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        #setting the datatype of the tensors that the model expects. Certain layers say embedding layer, requires the tensor to be of a specific datatype\n",
    "        #embedding layer needs the input to be of int\n",
    "        input_tensor = input_tensor.to(torch.int)\n",
    "        #label tensor required by the loss function is to be long\n",
    "        label = label.to(torch.long)\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        loss = loss_fn(output,label)\n",
    "        \n",
    "        #Hey! if you simply add loss and not loss.item() this causes a mem leak as the memory cannot be freed for the the next batch!\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b6867",
   "metadata": {
    "papermill": {
     "duration": 0.00791,
     "end_time": "2023-05-04T08:49:09.074234",
     "exception": false,
     "start_time": "2023-05-04T08:49:09.066324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Wuhunn!! now we can actually train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61c1ec",
   "metadata": {
    "papermill": {
     "duration": 0.007939,
     "end_time": "2023-05-04T08:49:09.090060",
     "exception": false,
     "start_time": "2023-05-04T08:49:09.082121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "lets have a sneak whether it actually works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b0d174b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:09.107935Z",
     "iopub.status.busy": "2023-05-04T08:49:09.107117Z",
     "iopub.status.idle": "2023-05-04T08:49:12.543812Z",
     "shell.execute_reply": "2023-05-04T08:49:12.542243Z"
    },
    "papermill": {
     "duration": 3.447925,
     "end_time": "2023-05-04T08:49:12.546111",
     "exception": false,
     "start_time": "2023-05-04T08:49:09.098186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.64071273803711\n"
     ]
    }
   ],
   "source": [
    "## test forward pass for one batch\n",
    "##having a sneak at one train batch\n",
    "\n",
    "for idx,data in enumerate(train_loader):\n",
    "    text_tensor,label = data\n",
    "    \n",
    "    text_tensor = text_tensor.to(device)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    text_tensor = text_tensor.to(torch.int)\n",
    "    label = label.to(torch.long)\n",
    "    output = model(text_tensor)\n",
    "    \n",
    "    loss_val = loss_fn(output,label)\n",
    "    \n",
    "    print(loss_val.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9be682f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:12.565047Z",
     "iopub.status.busy": "2023-05-04T08:49:12.564665Z",
     "iopub.status.idle": "2023-05-04T08:49:12.568854Z",
     "shell.execute_reply": "2023-05-04T08:49:12.567890Z"
    },
    "papermill": {
     "duration": 0.015629,
     "end_time": "2023-05-04T08:49:12.570765",
     "exception": false,
     "start_time": "2023-05-04T08:49:12.555136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab1b2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:12.589275Z",
     "iopub.status.busy": "2023-05-04T08:49:12.588558Z",
     "iopub.status.idle": "2023-05-04T08:49:12.699099Z",
     "shell.execute_reply": "2023-05-04T08:49:12.698250Z"
    },
    "papermill": {
     "duration": 0.121974,
     "end_time": "2023-05-04T08:49:12.701134",
     "exception": false,
     "start_time": "2023-05-04T08:49:12.579160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63a915c1",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-04T08:49:12.718876Z",
     "iopub.status.busy": "2023-05-04T08:49:12.718602Z",
     "iopub.status.idle": "2023-05-04T09:53:04.002979Z",
     "shell.execute_reply": "2023-05-04T09:53:04.001916Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 3831.296947,
     "end_time": "2023-05-04T09:53:04.006430",
     "exception": false,
     "start_time": "2023-05-04T08:49:12.709483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7653cb031a4330bf8517c1f967fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Epoch number 1\n",
      "  batch 1000 loss: 7.7959591114521025\n",
      "  batch 2000 loss: 10.701970691800117\n",
      "  batch 3000 loss: 13.107897161006928\n",
      "  batch 4000 loss: 15.47355836904049\n",
      "  batch 5000 loss: 17.804338939547538\n",
      "  batch 6000 loss: 20.133358375966548\n",
      "  batch 7000 loss: 22.470449255406855\n",
      "  batch 8000 loss: 24.78286001896858\n",
      "  batch 9000 loss: 27.125222016513348\n",
      "  batch 10000 loss: 29.430282545864582\n",
      "  batch 11000 loss: 31.718419974982737\n",
      "  batch 12000 loss: 34.031905772030356\n",
      "  batch 13000 loss: 36.3231982742548\n",
      "  batch 14000 loss: 38.59670258420706\n",
      "  batch 15000 loss: 40.89880152487755\n",
      "  batch 16000 loss: 43.16904085147381\n",
      "  batch 17000 loss: 45.42261055058241\n",
      "Validation loss for the epoch 2.2816945436404352\n",
      "Time take by the epoch 1 is 0:01:15.836876\n",
      "The Epoch number 2\n",
      "  batch 1000 loss: 2.2578606171011923\n",
      "  batch 2000 loss: 4.539325327813625\n",
      "  batch 3000 loss: 6.7889157776832585\n",
      "  batch 4000 loss: 9.01039670163393\n",
      "  batch 5000 loss: 11.256333523094654\n",
      "  batch 6000 loss: 13.495643796980382\n",
      "  batch 7000 loss: 15.733944802999497\n",
      "  batch 8000 loss: 17.98221812695265\n",
      "  batch 9000 loss: 20.175547891914846\n",
      "  batch 10000 loss: 22.4213842189312\n",
      "  batch 11000 loss: 24.650333433926104\n",
      "  batch 12000 loss: 26.842766482591628\n",
      "  batch 13000 loss: 29.032168776869774\n",
      "  batch 14000 loss: 31.228816672861576\n",
      "  batch 15000 loss: 33.41179619240761\n",
      "  batch 16000 loss: 35.60523577308655\n",
      "  batch 17000 loss: 37.81324066394568\n",
      "Validation loss for the epoch 2.210140733524008\n",
      "Time take by the epoch 2 is 0:01:16.781247\n",
      "The Epoch number 3\n",
      "  batch 1000 loss: 2.2059637717604637\n",
      "  batch 2000 loss: 4.38539505147934\n",
      "  batch 3000 loss: 6.562513938605785\n",
      "  batch 4000 loss: 8.751256266534329\n",
      "  batch 5000 loss: 10.916375119388103\n",
      "  batch 6000 loss: 13.045433060467243\n",
      "  batch 7000 loss: 15.187207674324512\n",
      "  batch 8000 loss: 17.35054012554884\n",
      "  batch 9000 loss: 19.500957116365434\n",
      "  batch 10000 loss: 21.648121618390082\n",
      "  batch 11000 loss: 23.76957195365429\n",
      "  batch 12000 loss: 25.892732286036015\n",
      "  batch 13000 loss: 28.045111757874487\n",
      "  batch 14000 loss: 30.14559213900566\n",
      "  batch 15000 loss: 32.26454798489809\n",
      "  batch 16000 loss: 34.400733097314834\n",
      "  batch 17000 loss: 36.51363285601139\n",
      "Validation loss for the epoch 2.1260537794685717\n",
      "Time take by the epoch 3 is 0:01:16.579196\n",
      "The Epoch number 4\n",
      "  batch 1000 loss: 2.112257778406143\n",
      "  batch 2000 loss: 4.203672005355358\n",
      "  batch 3000 loss: 6.259725299656391\n",
      "  batch 4000 loss: 8.340225620269775\n",
      "  batch 5000 loss: 10.400470348834991\n",
      "  batch 6000 loss: 12.441352726340295\n",
      "  batch 7000 loss: 14.49037417972088\n",
      "  batch 8000 loss: 16.516940261483192\n",
      "  batch 9000 loss: 18.55760292851925\n",
      "  batch 10000 loss: 20.593423222780228\n",
      "  batch 11000 loss: 22.61601810348034\n",
      "  batch 12000 loss: 24.62762397390604\n",
      "  batch 13000 loss: 26.64050462204218\n",
      "  batch 14000 loss: 28.65338302797079\n",
      "  batch 15000 loss: 30.660355469465255\n",
      "  batch 16000 loss: 32.64867449611425\n",
      "  batch 17000 loss: 34.632086140036584\n",
      "Validation loss for the epoch 1.985375216159591\n",
      "Time take by the epoch 4 is 0:01:16.769396\n",
      "The Epoch number 5\n",
      "  batch 1000 loss: 1.9537769436836243\n",
      "  batch 2000 loss: 3.898261363863945\n",
      "  batch 3000 loss: 5.817136628389359\n",
      "  batch 4000 loss: 7.745773978114128\n",
      "  batch 5000 loss: 9.674788409531116\n",
      "  batch 6000 loss: 11.561869285881519\n",
      "  batch 7000 loss: 13.44198880660534\n",
      "  batch 8000 loss: 15.325439888775348\n",
      "  batch 9000 loss: 17.192521766245367\n",
      "  batch 10000 loss: 19.057850133001804\n",
      "  batch 11000 loss: 20.923012652754785\n",
      "  batch 12000 loss: 22.75385075879097\n",
      "  batch 13000 loss: 24.606617304980755\n",
      "  batch 14000 loss: 26.41238375622034\n",
      "  batch 15000 loss: 28.232062843203543\n",
      "  batch 16000 loss: 30.054539431393145\n",
      "  batch 17000 loss: 31.87398385709524\n",
      "Validation loss for the epoch 1.8307193112776328\n",
      "Time take by the epoch 5 is 0:01:16.567430\n",
      "The Epoch number 6\n",
      "  batch 1000 loss: 1.8065427863001824\n",
      "  batch 2000 loss: 3.596668918669224\n",
      "  batch 3000 loss: 5.378969267189503\n",
      "  batch 4000 loss: 7.1472214270234105\n",
      "  batch 5000 loss: 8.922564414083958\n",
      "  batch 6000 loss: 10.696289306163788\n",
      "  batch 7000 loss: 12.454054627001286\n",
      "  batch 8000 loss: 14.21769746094942\n",
      "  batch 9000 loss: 15.994257907032967\n",
      "  batch 10000 loss: 17.771725343704222\n",
      "  batch 11000 loss: 19.50499827694893\n",
      "  batch 12000 loss: 21.234020086944103\n",
      "  batch 13000 loss: 22.964778740108013\n",
      "  batch 14000 loss: 24.699702232301235\n",
      "  batch 15000 loss: 26.433179483234884\n",
      "  batch 16000 loss: 28.164675620019437\n",
      "  batch 17000 loss: 29.915547548890114\n",
      "Validation loss for the epoch 1.7663866386255695\n",
      "Time take by the epoch 6 is 0:01:16.571879\n",
      "The Epoch number 7\n",
      "  batch 1000 loss: 1.7246117643117904\n",
      "  batch 2000 loss: 3.454777801156044\n",
      "  batch 3000 loss: 5.189685404479504\n",
      "  batch 4000 loss: 6.909868456304073\n",
      "  batch 5000 loss: 8.631177881658077\n",
      "  batch 6000 loss: 10.357846368014812\n",
      "  batch 7000 loss: 12.07870661497116\n",
      "  batch 8000 loss: 13.807130271017552\n",
      "  batch 9000 loss: 15.552314620494842\n",
      "  batch 10000 loss: 17.27013413387537\n",
      "  batch 11000 loss: 18.97766127169132\n",
      "  batch 12000 loss: 20.674903039216996\n",
      "  batch 13000 loss: 22.379539232850075\n",
      "  batch 14000 loss: 24.101542584896087\n",
      "  batch 15000 loss: 25.806250057280064\n",
      "  batch 16000 loss: 27.50243049675226\n",
      "  batch 17000 loss: 29.206947643041612\n",
      "Validation loss for the epoch 1.7452904176839001\n",
      "Time take by the epoch 7 is 0:01:16.632541\n",
      "The Epoch number 8\n",
      "  batch 1000 loss: 1.6966811992526054\n",
      "  batch 2000 loss: 3.4108020191192625\n",
      "  batch 3000 loss: 5.1221610934734345\n",
      "  batch 4000 loss: 6.835798459887505\n",
      "  batch 5000 loss: 8.544042593359947\n",
      "  batch 6000 loss: 10.260056636989116\n",
      "  batch 7000 loss: 11.955696130216122\n",
      "  batch 8000 loss: 13.66871856611967\n",
      "  batch 9000 loss: 15.3656266939044\n",
      "  batch 10000 loss: 17.049081065058708\n",
      "  batch 11000 loss: 18.734888540446757\n",
      "  batch 12000 loss: 20.432341812193393\n",
      "  batch 13000 loss: 22.133896789610386\n",
      "  batch 14000 loss: 23.837245630979538\n",
      "  batch 15000 loss: 25.529931290209294\n",
      "  batch 16000 loss: 27.235218859195708\n",
      "  batch 17000 loss: 28.914221277296544\n",
      "Validation loss for the epoch 1.734067938854419\n",
      "Time take by the epoch 8 is 0:01:16.665610\n",
      "The Epoch number 9\n",
      "  batch 1000 loss: 1.7000495525598527\n",
      "  batch 2000 loss: 3.392695625126362\n",
      "  batch 3000 loss: 5.0736851776838305\n",
      "  batch 4000 loss: 6.758296819388867\n",
      "  batch 5000 loss: 8.45905528396368\n",
      "  batch 6000 loss: 10.138359338223934\n",
      "  batch 7000 loss: 11.823708155870438\n",
      "  batch 8000 loss: 13.532431647598743\n",
      "  batch 9000 loss: 15.220161141037941\n",
      "  batch 10000 loss: 16.91214446270466\n",
      "  batch 11000 loss: 18.60925161629915\n",
      "  batch 12000 loss: 20.298028116464614\n",
      "  batch 13000 loss: 21.97216946977377\n",
      "  batch 14000 loss: 23.66665275913477\n",
      "  batch 15000 loss: 25.351000920951368\n",
      "  batch 16000 loss: 27.02238076132536\n",
      "  batch 17000 loss: 28.718054987847804\n",
      "Validation loss for the epoch 1.7252814205715739\n",
      "Time take by the epoch 9 is 0:01:16.665265\n",
      "The Epoch number 10\n",
      "  batch 1000 loss: 1.7011309708356857\n",
      "  batch 2000 loss: 3.3851920850872994\n",
      "  batch 3000 loss: 5.053756792366505\n",
      "  batch 4000 loss: 6.723761010050773\n",
      "  batch 5000 loss: 8.398362050831318\n",
      "  batch 6000 loss: 10.060530912339688\n",
      "  batch 7000 loss: 11.74425629377365\n",
      "  batch 8000 loss: 13.418255851447583\n",
      "  batch 9000 loss: 15.10380375725031\n",
      "  batch 10000 loss: 16.772261831760407\n",
      "  batch 11000 loss: 18.455751036465166\n",
      "  batch 12000 loss: 20.13755556154251\n",
      "  batch 13000 loss: 21.830480470597745\n",
      "  batch 14000 loss: 23.52677023434639\n",
      "  batch 15000 loss: 25.201555849075316\n",
      "  batch 16000 loss: 26.873554745793342\n",
      "  batch 17000 loss: 28.561979552447795\n",
      "Validation loss for the epoch 1.7188674077360573\n",
      "Time take by the epoch 10 is 0:01:16.546246\n",
      "The Epoch number 11\n",
      "  batch 1000 loss: 1.6815616567134857\n",
      "  batch 2000 loss: 3.357930859684944\n",
      "  batch 3000 loss: 5.031633947372437\n",
      "  batch 4000 loss: 6.692195987403393\n",
      "  batch 5000 loss: 8.371051522374152\n",
      "  batch 6000 loss: 10.037460507631302\n",
      "  batch 7000 loss: 11.714117639780044\n",
      "  batch 8000 loss: 13.396705845892429\n",
      "  batch 9000 loss: 15.05973890477419\n",
      "  batch 10000 loss: 16.73763709419966\n",
      "  batch 11000 loss: 18.408511554598807\n",
      "  batch 12000 loss: 20.08313229125738\n",
      "  batch 13000 loss: 21.751851688742637\n",
      "  batch 14000 loss: 23.423893841326237\n",
      "  batch 15000 loss: 25.09688007873297\n",
      "  batch 16000 loss: 26.766295549571513\n",
      "  batch 17000 loss: 28.433459983587266\n",
      "Validation loss for the epoch 1.7125921136501019\n",
      "Time take by the epoch 11 is 0:01:16.667516\n",
      "The Epoch number 12\n",
      "  batch 1000 loss: 1.6642760811448096\n",
      "  batch 2000 loss: 3.3291381855010984\n",
      "  batch 3000 loss: 4.992313686311245\n",
      "  batch 4000 loss: 6.672428395152092\n",
      "  batch 5000 loss: 8.33570711517334\n",
      "  batch 6000 loss: 9.996611107289791\n",
      "  batch 7000 loss: 11.673007620334625\n",
      "  batch 8000 loss: 13.32608166885376\n",
      "  batch 9000 loss: 14.991863163530827\n",
      "  batch 10000 loss: 16.658465032637118\n",
      "  batch 11000 loss: 18.330630728840827\n",
      "  batch 12000 loss: 19.99646906644106\n",
      "  batch 13000 loss: 21.66180206769705\n",
      "  batch 14000 loss: 23.336026186645032\n",
      "  batch 15000 loss: 24.998364464402197\n",
      "  batch 16000 loss: 26.65426561009884\n",
      "  batch 17000 loss: 28.327538244843485\n",
      "Validation loss for the epoch 1.7073628384914077\n",
      "Time take by the epoch 12 is 0:01:16.653107\n",
      "The Epoch number 13\n",
      "  batch 1000 loss: 1.677744520545006\n",
      "  batch 2000 loss: 3.326582504808903\n",
      "  batch 3000 loss: 4.983415307819843\n",
      "  batch 4000 loss: 6.65200422680378\n",
      "  batch 5000 loss: 8.32506981009245\n",
      "  batch 6000 loss: 9.979630852043629\n",
      "  batch 7000 loss: 11.645187328755856\n",
      "  batch 8000 loss: 13.298290404438973\n",
      "  batch 9000 loss: 14.958560460150242\n",
      "  batch 10000 loss: 16.60349427449703\n",
      "  batch 11000 loss: 18.262152490377424\n",
      "  batch 12000 loss: 19.925917446196078\n",
      "  batch 13000 loss: 21.570756357908248\n",
      "  batch 14000 loss: 23.23160558950901\n",
      "  batch 15000 loss: 24.88306087911129\n",
      "  batch 16000 loss: 26.556451471805573\n",
      "  batch 17000 loss: 28.214691482126714\n",
      "Validation loss for the epoch 1.703258060738289\n",
      "Time take by the epoch 13 is 0:01:16.640201\n",
      "The Epoch number 14\n",
      "  batch 1000 loss: 1.6426988182067872\n",
      "  batch 2000 loss: 3.2878351780176165\n",
      "  batch 3000 loss: 4.9576200808882716\n",
      "  batch 4000 loss: 6.627936239838601\n",
      "  batch 5000 loss: 8.293407904982567\n",
      "  batch 6000 loss: 9.946332725822925\n",
      "  batch 7000 loss: 11.611090410411357\n",
      "  batch 8000 loss: 13.264010878384113\n",
      "  batch 9000 loss: 14.924214964091778\n",
      "  batch 10000 loss: 16.57894835615158\n",
      "  batch 11000 loss: 18.23230859911442\n",
      "  batch 12000 loss: 19.865664465785027\n",
      "  batch 13000 loss: 21.532619038462638\n",
      "  batch 14000 loss: 23.191725854337214\n",
      "  batch 15000 loss: 24.83364472180605\n",
      "  batch 16000 loss: 26.487033574998378\n",
      "  batch 17000 loss: 28.126160507917405\n",
      "Validation loss for the epoch 1.6988868568472963\n",
      "Time take by the epoch 14 is 0:01:16.716562\n",
      "The Epoch number 15\n",
      "  batch 1000 loss: 1.6551114118695258\n",
      "  batch 2000 loss: 3.312019234597683\n",
      "  batch 3000 loss: 4.964076108872891\n",
      "  batch 4000 loss: 6.614317049980164\n",
      "  batch 5000 loss: 8.2697819160223\n",
      "  batch 6000 loss: 9.917600732088088\n",
      "  batch 7000 loss: 11.56413158762455\n",
      "  batch 8000 loss: 13.201204743623734\n",
      "  batch 9000 loss: 14.842596596360206\n",
      "  batch 10000 loss: 16.503232253432273\n",
      "  batch 11000 loss: 18.164637938916684\n",
      "  batch 12000 loss: 19.816267971932888\n",
      "  batch 13000 loss: 21.45554043740034\n",
      "  batch 14000 loss: 23.097461393892765\n",
      "  batch 15000 loss: 24.746175509274007\n",
      "  batch 16000 loss: 26.41129475235939\n",
      "  batch 17000 loss: 28.066998975396157\n",
      "Validation loss for the epoch 1.6948224653190804\n",
      "Time take by the epoch 15 is 0:01:16.612154\n",
      "The Epoch number 16\n",
      "  batch 1000 loss: 1.6471184507608414\n",
      "  batch 2000 loss: 3.288645778954029\n",
      "  batch 3000 loss: 4.934226181566715\n",
      "  batch 4000 loss: 6.585813302636146\n",
      "  batch 5000 loss: 8.23633899563551\n",
      "  batch 6000 loss: 9.887069670438766\n",
      "  batch 7000 loss: 11.541253947556019\n",
      "  batch 8000 loss: 13.198905682265758\n",
      "  batch 9000 loss: 14.838274075508117\n",
      "  batch 10000 loss: 16.470168394565583\n",
      "  batch 11000 loss: 18.102214155852796\n",
      "  batch 12000 loss: 19.747894622683525\n",
      "  batch 13000 loss: 21.387360385656358\n",
      "  batch 14000 loss: 23.045723525941373\n",
      "  batch 15000 loss: 24.698204290270805\n",
      "  batch 16000 loss: 26.32422013860941\n",
      "  batch 17000 loss: 27.97610307717323\n",
      "Validation loss for the epoch 1.691522049887309\n",
      "Time take by the epoch 16 is 0:01:16.653289\n",
      "The Epoch number 17\n",
      "  batch 1000 loss: 1.631213541805744\n",
      "  batch 2000 loss: 3.2552198711037637\n",
      "  batch 3000 loss: 4.906046038508415\n",
      "  batch 4000 loss: 6.564045612215995\n",
      "  batch 5000 loss: 8.211928520798683\n",
      "  batch 6000 loss: 9.838182315349579\n",
      "  batch 7000 loss: 11.496763752639295\n",
      "  batch 8000 loss: 13.147288576543332\n",
      "  batch 9000 loss: 14.78881637763977\n",
      "  batch 10000 loss: 16.414871672391893\n",
      "  batch 11000 loss: 18.059958848774432\n",
      "  batch 12000 loss: 19.6908696218133\n",
      "  batch 13000 loss: 21.33122530937195\n",
      "  batch 14000 loss: 22.971030335128308\n",
      "  batch 15000 loss: 24.616365655601026\n",
      "  batch 16000 loss: 26.275401076853274\n",
      "  batch 17000 loss: 27.90817142152786\n",
      "Validation loss for the epoch 1.6874492048775607\n",
      "Time take by the epoch 17 is 0:01:16.584801\n",
      "The Epoch number 18\n",
      "  batch 1000 loss: 1.6333912422060965\n",
      "  batch 2000 loss: 3.264935251533985\n",
      "  batch 3000 loss: 4.9004256299734115\n",
      "  batch 4000 loss: 6.568009622275829\n",
      "  batch 5000 loss: 8.209999311506747\n",
      "  batch 6000 loss: 9.859422267436981\n",
      "  batch 7000 loss: 11.509126663684844\n",
      "  batch 8000 loss: 13.136338181316853\n",
      "  batch 9000 loss: 14.766833230316639\n",
      "  batch 10000 loss: 16.415539279043674\n",
      "  batch 11000 loss: 18.048797583222388\n",
      "  batch 12000 loss: 19.672160265445708\n",
      "  batch 13000 loss: 21.29433536529541\n",
      "  batch 14000 loss: 22.925057397902012\n",
      "  batch 15000 loss: 24.564307559669018\n",
      "  batch 16000 loss: 26.206077371001243\n",
      "  batch 17000 loss: 27.832254303455354\n",
      "Validation loss for the epoch 1.6840664214849417\n",
      "Time take by the epoch 18 is 0:01:16.775810\n",
      "The Epoch number 19\n",
      "  batch 1000 loss: 1.6271933715939522\n",
      "  batch 2000 loss: 3.249426330804825\n",
      "  batch 3000 loss: 4.881703797161579\n",
      "  batch 4000 loss: 6.525880539596081\n",
      "  batch 5000 loss: 8.173179355680942\n",
      "  batch 6000 loss: 9.808644402205944\n",
      "  batch 7000 loss: 11.435460713982582\n",
      "  batch 8000 loss: 13.065862468957901\n",
      "  batch 9000 loss: 14.700390435159207\n",
      "  batch 10000 loss: 16.330966716408728\n",
      "  batch 11000 loss: 17.974084180295467\n",
      "  batch 12000 loss: 19.61033219385147\n",
      "  batch 13000 loss: 21.22959517788887\n",
      "  batch 14000 loss: 22.864369496643544\n",
      "  batch 15000 loss: 24.512221431553364\n",
      "  batch 16000 loss: 26.149360873281957\n",
      "  batch 17000 loss: 27.79204985433817\n",
      "Validation loss for the epoch 1.6811000075321503\n",
      "Time take by the epoch 19 is 0:01:16.562424\n",
      "The Epoch number 20\n",
      "  batch 1000 loss: 1.620165514945984\n",
      "  batch 2000 loss: 3.251118878722191\n",
      "  batch 3000 loss: 4.862947389185429\n",
      "  batch 4000 loss: 6.504790615856647\n",
      "  batch 5000 loss: 8.13411145812273\n",
      "  batch 6000 loss: 9.772767892301083\n",
      "  batch 7000 loss: 11.40187590175867\n",
      "  batch 8000 loss: 13.022896206080913\n",
      "  batch 9000 loss: 14.654550591349603\n",
      "  batch 10000 loss: 16.281751605987548\n",
      "  batch 11000 loss: 17.927898837625982\n",
      "  batch 12000 loss: 19.57167657017708\n",
      "  batch 13000 loss: 21.223356865942478\n",
      "  batch 14000 loss: 22.84806890273094\n",
      "  batch 15000 loss: 24.478809158623218\n",
      "  batch 16000 loss: 26.110147903800012\n",
      "  batch 17000 loss: 27.72925333815813\n",
      "Validation loss for the epoch 1.6783706683697428\n",
      "Time take by the epoch 20 is 0:01:16.603082\n",
      "The Epoch number 21\n",
      "  batch 1000 loss: 1.6097033720612526\n",
      "  batch 2000 loss: 3.2205564205646513\n",
      "  batch 3000 loss: 4.844511654496193\n",
      "  batch 4000 loss: 6.468968678534031\n",
      "  batch 5000 loss: 8.111979245364665\n",
      "  batch 6000 loss: 9.752326793730258\n",
      "  batch 7000 loss: 11.389700137317181\n",
      "  batch 8000 loss: 13.028595719873906\n",
      "  batch 9000 loss: 14.64448292762041\n",
      "  batch 10000 loss: 16.282146358013154\n",
      "  batch 11000 loss: 17.907459638357164\n",
      "  batch 12000 loss: 19.538818779587746\n",
      "  batch 13000 loss: 21.17227147769928\n",
      "  batch 14000 loss: 22.79332704424858\n",
      "  batch 15000 loss: 24.4169748519063\n",
      "  batch 16000 loss: 26.05975011527538\n",
      "  batch 17000 loss: 27.689935667693614\n",
      "Validation loss for the epoch 1.6754104170873227\n",
      "Time take by the epoch 21 is 0:01:16.750061\n",
      "The Epoch number 22\n",
      "  batch 1000 loss: 1.6237897678613662\n",
      "  batch 2000 loss: 3.234478475153446\n",
      "  batch 3000 loss: 4.848491128146648\n",
      "  batch 4000 loss: 6.468695782005787\n",
      "  batch 5000 loss: 8.090566640615464\n",
      "  batch 6000 loss: 9.727123995423318\n",
      "  batch 7000 loss: 11.361442383170127\n",
      "  batch 8000 loss: 13.001688349187374\n",
      "  batch 9000 loss: 14.624645994007587\n",
      "  batch 10000 loss: 16.257248174369334\n",
      "  batch 11000 loss: 17.884849715173246\n",
      "  batch 12000 loss: 19.520287427067757\n",
      "  batch 13000 loss: 21.146666677594183\n",
      "  batch 14000 loss: 22.767520097494124\n",
      "  batch 15000 loss: 24.382827192723752\n",
      "  batch 16000 loss: 25.997700382947922\n",
      "  batch 17000 loss: 27.631459081947803\n",
      "Validation loss for the epoch 1.6727601083292678\n",
      "Time take by the epoch 22 is 0:01:16.531416\n",
      "The Epoch number 23\n",
      "  batch 1000 loss: 1.6372057893276215\n",
      "  batch 2000 loss: 3.258982309281826\n",
      "  batch 3000 loss: 4.8892193005681035\n",
      "  batch 4000 loss: 6.50804516774416\n",
      "  batch 5000 loss: 8.121805492281913\n",
      "  batch 6000 loss: 9.735440025866032\n",
      "  batch 7000 loss: 11.360269233047962\n",
      "  batch 8000 loss: 12.98473175561428\n",
      "  batch 9000 loss: 14.621602511584758\n",
      "  batch 10000 loss: 16.255617215394974\n",
      "  batch 11000 loss: 17.864403783082963\n",
      "  batch 12000 loss: 19.49672294175625\n",
      "  batch 13000 loss: 21.109051192104815\n",
      "  batch 14000 loss: 22.711693137347698\n",
      "  batch 15000 loss: 24.32601043790579\n",
      "  batch 16000 loss: 25.953661514937878\n",
      "  batch 17000 loss: 27.58399793821573\n",
      "Validation loss for the epoch 1.6712305310507234\n",
      "Time take by the epoch 23 is 0:01:16.670749\n",
      "The Epoch number 24\n",
      "  batch 1000 loss: 1.6154199591875076\n",
      "  batch 2000 loss: 3.2204840757846833\n",
      "  batch 3000 loss: 4.8278415275812145\n",
      "  batch 4000 loss: 6.4256173098087315\n",
      "  batch 5000 loss: 8.07524086034298\n",
      "  batch 6000 loss: 9.717956184983253\n",
      "  batch 7000 loss: 11.349469924509526\n",
      "  batch 8000 loss: 12.95939458733797\n",
      "  batch 9000 loss: 14.567729807555676\n",
      "  batch 10000 loss: 16.197674961209298\n",
      "  batch 11000 loss: 17.819421190321446\n",
      "  batch 12000 loss: 19.43291933184862\n",
      "  batch 13000 loss: 21.057539714992046\n",
      "  batch 14000 loss: 22.674377268075943\n",
      "  batch 15000 loss: 24.291130118906498\n",
      "  batch 16000 loss: 25.921064908444883\n",
      "  batch 17000 loss: 27.546309255480768\n",
      "Validation loss for the epoch 1.6697719073400477\n",
      "Time take by the epoch 24 is 0:01:16.535939\n",
      "The Epoch number 25\n",
      "  batch 1000 loss: 1.6195948948860168\n",
      "  batch 2000 loss: 3.234032816529274\n",
      "  batch 3000 loss: 4.849496069312096\n",
      "  batch 4000 loss: 6.490033035159111\n",
      "  batch 5000 loss: 8.11185148048401\n",
      "  batch 6000 loss: 9.734613274157047\n",
      "  batch 7000 loss: 11.32919034320116\n",
      "  batch 8000 loss: 12.94355244588852\n",
      "  batch 9000 loss: 14.562663395464421\n",
      "  batch 10000 loss: 16.178743690371512\n",
      "  batch 11000 loss: 17.796565622866154\n",
      "  batch 12000 loss: 19.415640511870386\n",
      "  batch 13000 loss: 21.04278507345915\n",
      "  batch 14000 loss: 22.658028314173222\n",
      "  batch 15000 loss: 24.256754429638384\n",
      "  batch 16000 loss: 25.873379219293593\n",
      "  batch 17000 loss: 27.494994870901106\n",
      "Validation loss for the epoch 1.666076510171698\n",
      "Time take by the epoch 25 is 0:01:16.607500\n",
      "The Epoch number 26\n",
      "  batch 1000 loss: 1.6108507959842682\n",
      "  batch 2000 loss: 3.220898838877678\n",
      "  batch 3000 loss: 4.862860352873803\n",
      "  batch 4000 loss: 6.473022411108017\n",
      "  batch 5000 loss: 8.085824609458447\n",
      "  batch 6000 loss: 9.696940634727477\n",
      "  batch 7000 loss: 11.305848983228207\n",
      "  batch 8000 loss: 12.928351335287093\n",
      "  batch 9000 loss: 14.558963443756104\n",
      "  batch 10000 loss: 16.171108731508255\n",
      "  batch 11000 loss: 17.774342533409595\n",
      "  batch 12000 loss: 19.379360418498518\n",
      "  batch 13000 loss: 21.008732960283755\n",
      "  batch 14000 loss: 22.623893624842168\n",
      "  batch 15000 loss: 24.208214635789396\n",
      "  batch 16000 loss: 25.845536444723606\n",
      "  batch 17000 loss: 27.45404177558422\n",
      "Validation loss for the epoch 1.6637766463360761\n",
      "Time take by the epoch 26 is 0:01:16.608864\n",
      "The Epoch number 27\n",
      "  batch 1000 loss: 1.6134459875226022\n",
      "  batch 2000 loss: 3.2499412356615065\n",
      "  batch 3000 loss: 4.839961315155029\n",
      "  batch 4000 loss: 6.463152864038944\n",
      "  batch 5000 loss: 8.068699783802032\n",
      "  batch 6000 loss: 9.68663518410921\n",
      "  batch 7000 loss: 11.284029348015785\n",
      "  batch 8000 loss: 12.902359751820564\n",
      "  batch 9000 loss: 14.500410557985306\n",
      "  batch 10000 loss: 16.119989538133144\n",
      "  batch 11000 loss: 17.73829217851162\n",
      "  batch 12000 loss: 19.35198474729061\n",
      "  batch 13000 loss: 20.964444330275057\n",
      "  batch 14000 loss: 22.58955728960037\n",
      "  batch 15000 loss: 24.208156820595264\n",
      "  batch 16000 loss: 25.8083895432353\n",
      "  batch 17000 loss: 27.415729588449\n",
      "Validation loss for the epoch 1.6620251230686787\n",
      "Time take by the epoch 27 is 0:01:16.575594\n",
      "The Epoch number 28\n",
      "  batch 1000 loss: 1.6040785669088364\n",
      "  batch 2000 loss: 3.213429346740246\n",
      "  batch 3000 loss: 4.803914249658584\n",
      "  batch 4000 loss: 6.431536978662014\n",
      "  batch 5000 loss: 8.061940490424632\n",
      "  batch 6000 loss: 9.672524611949921\n",
      "  batch 7000 loss: 11.278120014309883\n",
      "  batch 8000 loss: 12.900759118139744\n",
      "  batch 9000 loss: 14.515899633526802\n",
      "  batch 10000 loss: 16.148361333251\n",
      "  batch 11000 loss: 17.746378657221793\n",
      "  batch 12000 loss: 19.357950340032577\n",
      "  batch 13000 loss: 20.975507023513316\n",
      "  batch 14000 loss: 22.59106136369705\n",
      "  batch 15000 loss: 24.194386547386646\n",
      "  batch 16000 loss: 25.782131975471973\n",
      "  batch 17000 loss: 27.37952343508601\n",
      "Validation loss for the epoch 1.660228908082682\n",
      "Time take by the epoch 28 is 0:01:16.700266\n",
      "The Epoch number 29\n",
      "  batch 1000 loss: 1.601492874443531\n",
      "  batch 2000 loss: 3.2165018062591555\n",
      "  batch 3000 loss: 4.815910751760006\n",
      "  batch 4000 loss: 6.414031403183937\n",
      "  batch 5000 loss: 8.02607413882017\n",
      "  batch 6000 loss: 9.645707093954087\n",
      "  batch 7000 loss: 11.251371335625649\n",
      "  batch 8000 loss: 12.886099158704281\n",
      "  batch 9000 loss: 14.506146266043187\n",
      "  batch 10000 loss: 16.107554810643197\n",
      "  batch 11000 loss: 17.71278504496813\n",
      "  batch 12000 loss: 19.310867769479753\n",
      "  batch 13000 loss: 20.92300754714012\n",
      "  batch 14000 loss: 22.53560110926628\n",
      "  batch 15000 loss: 24.1408747907877\n",
      "  batch 16000 loss: 25.735701042473316\n",
      "  batch 17000 loss: 27.336266186356543\n",
      "Validation loss for the epoch 1.6578917252738654\n",
      "Time take by the epoch 29 is 0:01:16.551954\n",
      "The Epoch number 30\n",
      "  batch 1000 loss: 1.611330647468567\n",
      "  batch 2000 loss: 3.2098688274621963\n",
      "  batch 3000 loss: 4.80930558860302\n",
      "  batch 4000 loss: 6.409014616906643\n",
      "  batch 5000 loss: 8.020541317284108\n",
      "  batch 6000 loss: 9.619682499527931\n",
      "  batch 7000 loss: 11.235763107776641\n",
      "  batch 8000 loss: 12.832958611965179\n",
      "  batch 9000 loss: 14.421168998241425\n",
      "  batch 10000 loss: 16.021743944346905\n",
      "  batch 11000 loss: 17.634273966610433\n",
      "  batch 12000 loss: 19.2376327098608\n",
      "  batch 13000 loss: 20.856891310334206\n",
      "  batch 14000 loss: 22.454692681252958\n",
      "  batch 15000 loss: 24.055376808941364\n",
      "  batch 16000 loss: 25.676253434240817\n",
      "  batch 17000 loss: 27.295995193004607\n",
      "Validation loss for the epoch 1.6559774780692975\n",
      "Time take by the epoch 30 is 0:01:16.675316\n",
      "The Epoch number 31\n",
      "  batch 1000 loss: 1.581950329720974\n",
      "  batch 2000 loss: 3.191649021267891\n",
      "  batch 3000 loss: 4.800820053935051\n",
      "  batch 4000 loss: 6.400908416807652\n",
      "  batch 5000 loss: 8.00530262929201\n",
      "  batch 6000 loss: 9.606092139959335\n",
      "  batch 7000 loss: 11.225302874565125\n",
      "  batch 8000 loss: 12.839506597042083\n",
      "  batch 9000 loss: 14.453650310337544\n",
      "  batch 10000 loss: 16.061048900544645\n",
      "  batch 11000 loss: 17.666810247957706\n",
      "  batch 12000 loss: 19.258135928988455\n",
      "  batch 13000 loss: 20.860838423073293\n",
      "  batch 14000 loss: 22.4676325584054\n",
      "  batch 15000 loss: 24.081203864991664\n",
      "  batch 16000 loss: 25.68582731604576\n",
      "  batch 17000 loss: 27.288674148857595\n",
      "Validation loss for the epoch 1.6542148839878137\n",
      "Time take by the epoch 31 is 0:01:16.683428\n",
      "The Epoch number 32\n",
      "  batch 1000 loss: 1.5963299050927162\n",
      "  batch 2000 loss: 3.2014047339558602\n",
      "  batch 3000 loss: 4.831151262760162\n",
      "  batch 4000 loss: 6.432282324433327\n",
      "  batch 5000 loss: 8.022254325807095\n",
      "  batch 6000 loss: 9.617336510002612\n",
      "  batch 7000 loss: 11.237189373612404\n",
      "  batch 8000 loss: 12.815528738230467\n",
      "  batch 9000 loss: 14.432497680187225\n",
      "  batch 10000 loss: 16.02659702581167\n",
      "  batch 11000 loss: 17.600350395977497\n",
      "  batch 12000 loss: 19.213473489940167\n",
      "  batch 13000 loss: 20.822805749595165\n",
      "  batch 14000 loss: 22.418601423859595\n",
      "  batch 15000 loss: 24.014019975721837\n",
      "  batch 16000 loss: 25.631021268844606\n",
      "  batch 17000 loss: 27.2335517873168\n",
      "Validation loss for the epoch 1.6532521253660006\n",
      "Time take by the epoch 32 is 0:01:16.606130\n",
      "The Epoch number 33\n",
      "  batch 1000 loss: 1.6079649446010589\n",
      "  batch 2000 loss: 3.1852888649106026\n",
      "  batch 3000 loss: 4.792461459934711\n",
      "  batch 4000 loss: 6.412175943136215\n",
      "  batch 5000 loss: 8.010108029901982\n",
      "  batch 6000 loss: 9.58190715199709\n",
      "  batch 7000 loss: 11.19143160176277\n",
      "  batch 8000 loss: 12.77856769901514\n",
      "  batch 9000 loss: 14.370344544708729\n",
      "  batch 10000 loss: 15.974169258832932\n",
      "  batch 11000 loss: 17.56719285684824\n",
      "  batch 12000 loss: 19.169697263360025\n",
      "  batch 13000 loss: 20.7792872248888\n",
      "  batch 14000 loss: 22.38683429145813\n",
      "  batch 15000 loss: 24.009303864359854\n",
      "  batch 16000 loss: 25.60303931993246\n",
      "  batch 17000 loss: 27.215483360916377\n",
      "Validation loss for the epoch 1.6508781275254594\n",
      "Time take by the epoch 33 is 0:01:16.679936\n",
      "The Epoch number 34\n",
      "  batch 1000 loss: 1.5870501492023468\n",
      "  batch 2000 loss: 3.2003857607245445\n",
      "  batch 3000 loss: 4.829485027194023\n",
      "  batch 4000 loss: 6.435917163342237\n",
      "  batch 5000 loss: 8.041844094008207\n",
      "  batch 6000 loss: 9.649296944588423\n",
      "  batch 7000 loss: 11.243719872742892\n",
      "  batch 8000 loss: 12.836153670638799\n",
      "  batch 9000 loss: 14.426588464349509\n",
      "  batch 10000 loss: 16.005136943370104\n",
      "  batch 11000 loss: 17.603289251059294\n",
      "  batch 12000 loss: 19.206620152920486\n",
      "  batch 13000 loss: 20.784423650711776\n",
      "  batch 14000 loss: 22.39480963423848\n",
      "  batch 15000 loss: 23.982268262714147\n",
      "  batch 16000 loss: 25.588692727416753\n",
      "  batch 17000 loss: 27.19594194123149\n",
      "Validation loss for the epoch 1.64935970318817\n",
      "Time take by the epoch 34 is 0:01:16.561406\n",
      "The Epoch number 35\n",
      "  batch 1000 loss: 1.5633503423929214\n",
      "  batch 2000 loss: 3.1507564820051193\n",
      "  batch 3000 loss: 4.733661142766476\n",
      "  batch 4000 loss: 6.332758109092713\n",
      "  batch 5000 loss: 7.935700471937657\n",
      "  batch 6000 loss: 9.55031237053871\n",
      "  batch 7000 loss: 11.154739647805691\n",
      "  batch 8000 loss: 12.75318468901515\n",
      "  batch 9000 loss: 14.349208912223578\n",
      "  batch 10000 loss: 15.97019324144721\n",
      "  batch 11000 loss: 17.56780946931243\n",
      "  batch 12000 loss: 19.180280217319726\n",
      "  batch 13000 loss: 20.768564295083284\n",
      "  batch 14000 loss: 22.377820302575827\n",
      "  batch 15000 loss: 23.966106046050786\n",
      "  batch 16000 loss: 25.572448547452687\n",
      "  batch 17000 loss: 27.166744832783937\n",
      "Validation loss for the epoch 1.6490231496533134\n",
      "Time take by the epoch 35 is 0:01:16.792077\n",
      "The Epoch number 36\n",
      "  batch 1000 loss: 1.5877888216376304\n",
      "  batch 2000 loss: 3.18490634649992\n",
      "  batch 3000 loss: 4.772469651460647\n",
      "  batch 4000 loss: 6.359593391656875\n",
      "  batch 5000 loss: 7.940484117031097\n",
      "  batch 6000 loss: 9.541098373293877\n",
      "  batch 7000 loss: 11.130747653603553\n",
      "  batch 8000 loss: 12.744027754366398\n",
      "  batch 9000 loss: 14.32662838357687\n",
      "  batch 10000 loss: 15.930941117703915\n",
      "  batch 11000 loss: 17.5450095154047\n",
      "  batch 12000 loss: 19.151832120358943\n",
      "  batch 13000 loss: 20.734531125724317\n",
      "  batch 14000 loss: 22.341695305526258\n",
      "  batch 15000 loss: 23.934951233625412\n",
      "  batch 16000 loss: 25.536025507628917\n",
      "  batch 17000 loss: 27.130594883322715\n",
      "Validation loss for the epoch 1.647785762044174\n",
      "Time take by the epoch 36 is 0:01:16.576648\n",
      "The Epoch number 37\n",
      "  batch 1000 loss: 1.605857416689396\n",
      "  batch 2000 loss: 3.202200452387333\n",
      "  batch 3000 loss: 4.786745620965958\n",
      "  batch 4000 loss: 6.3874553805589676\n",
      "  batch 5000 loss: 8.012616771399975\n",
      "  batch 6000 loss: 9.58319021999836\n",
      "  batch 7000 loss: 11.167425483822823\n",
      "  batch 8000 loss: 12.755020492494106\n",
      "  batch 9000 loss: 14.32468538558483\n",
      "  batch 10000 loss: 15.92955742418766\n",
      "  batch 11000 loss: 17.521372197270395\n",
      "  batch 12000 loss: 19.12324538320303\n",
      "  batch 13000 loss: 20.710183994710444\n",
      "  batch 14000 loss: 22.31324821668863\n",
      "  batch 15000 loss: 23.91437572926283\n",
      "  batch 16000 loss: 25.509952464520932\n",
      "  batch 17000 loss: 27.118203844845294\n",
      "Validation loss for the epoch 1.646311188439689\n",
      "Time take by the epoch 37 is 0:01:16.663660\n",
      "The Epoch number 38\n",
      "  batch 1000 loss: 1.5965516724586486\n",
      "  batch 2000 loss: 3.1695149690508844\n",
      "  batch 3000 loss: 4.748512112319469\n",
      "  batch 4000 loss: 6.343856045842171\n",
      "  batch 5000 loss: 7.946804384887218\n",
      "  batch 6000 loss: 9.54409542065859\n",
      "  batch 7000 loss: 11.129448527336121\n",
      "  batch 8000 loss: 12.726754838764668\n",
      "  batch 9000 loss: 14.337223846197128\n",
      "  batch 10000 loss: 15.942032311558723\n",
      "  batch 11000 loss: 17.557140819877386\n",
      "  batch 12000 loss: 19.13785640385747\n",
      "  batch 13000 loss: 20.745745911866425\n",
      "  batch 14000 loss: 22.340988639980555\n",
      "  batch 15000 loss: 23.913829954952\n",
      "  batch 16000 loss: 25.503871774226425\n",
      "  batch 17000 loss: 27.096905476897955\n",
      "Validation loss for the epoch 1.6457178996141433\n",
      "Time take by the epoch 38 is 0:01:16.652719\n",
      "The Epoch number 39\n",
      "  batch 1000 loss: 1.5930488831996918\n",
      "  batch 2000 loss: 3.1811797282099725\n",
      "  batch 3000 loss: 4.779329273819924\n",
      "  batch 4000 loss: 6.363417177677155\n",
      "  batch 5000 loss: 7.946601130008697\n",
      "  batch 6000 loss: 9.5359144423604\n",
      "  batch 7000 loss: 11.107073052167893\n",
      "  batch 8000 loss: 12.691315464138984\n",
      "  batch 9000 loss: 14.2883899217844\n",
      "  batch 10000 loss: 15.90399351990223\n",
      "  batch 11000 loss: 17.48536665213108\n",
      "  batch 12000 loss: 19.08695429968834\n",
      "  batch 13000 loss: 20.70029450362921\n",
      "  batch 14000 loss: 22.287121236681937\n",
      "  batch 15000 loss: 23.881784621596335\n",
      "  batch 16000 loss: 25.48523284393549\n",
      "  batch 17000 loss: 27.0643358142972\n",
      "Validation loss for the epoch 1.6441037534811784\n",
      "Time take by the epoch 39 is 0:01:16.647094\n",
      "The Epoch number 40\n",
      "  batch 1000 loss: 1.5860152996778487\n",
      "  batch 2000 loss: 3.1690950539708136\n",
      "  batch 3000 loss: 4.765497194945812\n",
      "  batch 4000 loss: 6.358981827795506\n",
      "  batch 5000 loss: 7.966535226166249\n",
      "  batch 6000 loss: 9.554378388941288\n",
      "  batch 7000 loss: 11.141597464859485\n",
      "  batch 8000 loss: 12.73573648160696\n",
      "  batch 9000 loss: 14.347954674243926\n",
      "  batch 10000 loss: 15.906786724448205\n",
      "  batch 11000 loss: 17.50588224321604\n",
      "  batch 12000 loss: 19.113993603527547\n",
      "  batch 13000 loss: 20.71412621319294\n",
      "  batch 14000 loss: 22.305751233637334\n",
      "  batch 15000 loss: 23.888553282499313\n",
      "  batch 16000 loss: 25.48966907835007\n",
      "  batch 17000 loss: 27.05325845092535\n",
      "Validation loss for the epoch 1.6431431307729734\n",
      "Time take by the epoch 40 is 0:01:16.684036\n",
      "The Epoch number 41\n",
      "  batch 1000 loss: 1.5998999227285384\n",
      "  batch 2000 loss: 3.1889841072559357\n",
      "  batch 3000 loss: 4.787008256316185\n",
      "  batch 4000 loss: 6.348163597345352\n",
      "  batch 5000 loss: 7.940815352261066\n",
      "  batch 6000 loss: 9.51227763134241\n",
      "  batch 7000 loss: 11.110156831502914\n",
      "  batch 8000 loss: 12.709491747111082\n",
      "  batch 9000 loss: 14.296967559427022\n",
      "  batch 10000 loss: 15.900205408483744\n",
      "  batch 11000 loss: 17.50054336974025\n",
      "  batch 12000 loss: 19.090093969672917\n",
      "  batch 13000 loss: 20.676785354167222\n",
      "  batch 14000 loss: 22.261088967114688\n",
      "  batch 15000 loss: 23.84863012227416\n",
      "  batch 16000 loss: 25.434400650411845\n",
      "  batch 17000 loss: 27.037329273492098\n",
      "Validation loss for the epoch 1.641289436201664\n",
      "Time take by the epoch 41 is 0:01:16.538991\n",
      "The Epoch number 42\n",
      "  batch 1000 loss: 1.59103225928545\n",
      "  batch 2000 loss: 3.175331136882305\n",
      "  batch 3000 loss: 4.771610831320285\n",
      "  batch 4000 loss: 6.36342919921875\n",
      "  batch 5000 loss: 7.946153271436692\n",
      "  batch 6000 loss: 9.523324550271035\n",
      "  batch 7000 loss: 11.113792625784875\n",
      "  batch 8000 loss: 12.698511468559504\n",
      "  batch 9000 loss: 14.283243084818125\n",
      "  batch 10000 loss: 15.875331966787577\n",
      "  batch 11000 loss: 17.47594823846221\n",
      "  batch 12000 loss: 19.059319420814514\n",
      "  batch 13000 loss: 20.64886727875471\n",
      "  batch 14000 loss: 22.241739683806895\n",
      "  batch 15000 loss: 23.826417994737625\n",
      "  batch 16000 loss: 25.429713336884976\n",
      "  batch 17000 loss: 27.02152290678024\n",
      "Validation loss for the epoch 1.641513626866386\n",
      "Time take by the epoch 42 is 0:01:16.861914\n",
      "The Epoch number 43\n",
      "  batch 1000 loss: 1.5960619063973427\n",
      "  batch 2000 loss: 3.174987750649452\n",
      "  batch 3000 loss: 4.761371659994126\n",
      "  batch 4000 loss: 6.344202151834965\n",
      "  batch 5000 loss: 7.949870007097721\n",
      "  batch 6000 loss: 9.540025517165661\n",
      "  batch 7000 loss: 11.146680395662784\n",
      "  batch 8000 loss: 12.72878867149353\n",
      "  batch 9000 loss: 14.299492604017258\n",
      "  batch 10000 loss: 15.87281356048584\n",
      "  batch 11000 loss: 17.46071551015973\n",
      "  batch 12000 loss: 19.060114161998033\n",
      "  batch 13000 loss: 20.64185812160373\n",
      "  batch 14000 loss: 22.241964602559804\n",
      "  batch 15000 loss: 23.82293260577321\n",
      "  batch 16000 loss: 25.402108364492655\n",
      "  batch 17000 loss: 26.98934739461541\n",
      "Validation loss for the epoch 1.6403348764409398\n",
      "Time take by the epoch 43 is 0:01:16.591268\n",
      "The Epoch number 44\n",
      "  batch 1000 loss: 1.6005727082490921\n",
      "  batch 2000 loss: 3.1712131947875024\n",
      "  batch 3000 loss: 4.7587764807343484\n",
      "  batch 4000 loss: 6.341289125680923\n",
      "  batch 5000 loss: 7.932429222583771\n",
      "  batch 6000 loss: 9.531200018286706\n",
      "  batch 7000 loss: 11.111609693408012\n",
      "  batch 8000 loss: 12.701260208427906\n",
      "  batch 9000 loss: 14.2731421995759\n",
      "  batch 10000 loss: 15.868776398837566\n",
      "  batch 11000 loss: 17.477835265636443\n",
      "  batch 12000 loss: 19.047404575288297\n",
      "  batch 13000 loss: 20.633910876333715\n",
      "  batch 14000 loss: 22.241869602560996\n",
      "  batch 15000 loss: 23.824814561605454\n",
      "  batch 16000 loss: 25.404571845293045\n",
      "  batch 17000 loss: 26.98075131946802\n",
      "Validation loss for the epoch 1.6395998138414265\n",
      "Time take by the epoch 44 is 0:01:16.529408\n",
      "The Epoch number 45\n",
      "  batch 1000 loss: 1.5883820583224297\n",
      "  batch 2000 loss: 3.1861272964179514\n",
      "  batch 3000 loss: 4.76196335029602\n",
      "  batch 4000 loss: 6.357188543260097\n",
      "  batch 5000 loss: 7.940013165265322\n",
      "  batch 6000 loss: 9.526576183825732\n",
      "  batch 7000 loss: 11.093324969559909\n",
      "  batch 8000 loss: 12.684124945729971\n",
      "  batch 9000 loss: 14.261195618063212\n",
      "  batch 10000 loss: 15.831735419005156\n",
      "  batch 11000 loss: 17.434100542843343\n",
      "  batch 12000 loss: 19.01884289777279\n",
      "  batch 13000 loss: 20.606850476384164\n",
      "  batch 14000 loss: 22.190637321650982\n",
      "  batch 15000 loss: 23.766863481104373\n",
      "  batch 16000 loss: 25.373009869635105\n",
      "  batch 17000 loss: 26.967172105312347\n",
      "Validation loss for the epoch 1.6401097806936946\n",
      "Time take by the epoch 45 is 0:01:16.686748\n",
      "The Epoch number 46\n",
      "  batch 1000 loss: 1.551446589231491\n",
      "  batch 2000 loss: 3.114219158887863\n",
      "  batch 3000 loss: 4.693374968647957\n",
      "  batch 4000 loss: 6.26800719833374\n",
      "  batch 5000 loss: 7.857965824574232\n",
      "  batch 6000 loss: 9.44744589868188\n",
      "  batch 7000 loss: 11.01412292328477\n",
      "  batch 8000 loss: 12.59678443261981\n",
      "  batch 9000 loss: 14.192428357094526\n",
      "  batch 10000 loss: 15.791919320970774\n",
      "  batch 11000 loss: 17.378954259425402\n",
      "  batch 12000 loss: 18.967631035238504\n",
      "  batch 13000 loss: 20.570451435238123\n",
      "  batch 14000 loss: 22.136335782974957\n",
      "  batch 15000 loss: 23.732639062851668\n",
      "  batch 16000 loss: 25.35245704588294\n",
      "  batch 17000 loss: 26.93581885614991\n",
      "Validation loss for the epoch 1.6379714711972702\n",
      "Time take by the epoch 46 is 0:01:16.590040\n",
      "The Epoch number 47\n",
      "  batch 1000 loss: 1.5824322090744973\n",
      "  batch 2000 loss: 3.1483529544472693\n",
      "  batch 3000 loss: 4.730308966934681\n",
      "  batch 4000 loss: 6.3036298564076425\n",
      "  batch 5000 loss: 7.88314858430624\n",
      "  batch 6000 loss: 9.473019228339195\n",
      "  batch 7000 loss: 11.045232206344604\n",
      "  batch 8000 loss: 12.624559997022152\n",
      "  batch 9000 loss: 14.201872505843639\n",
      "  batch 10000 loss: 15.799293674051762\n",
      "  batch 11000 loss: 17.41018283766508\n",
      "  batch 12000 loss: 19.013073329031467\n",
      "  batch 13000 loss: 20.588670893609525\n",
      "  batch 14000 loss: 22.179468243420125\n",
      "  batch 15000 loss: 23.75523025393486\n",
      "  batch 16000 loss: 25.343497975260018\n",
      "  batch 17000 loss: 26.924828666061163\n",
      "Validation loss for the epoch 1.6370001124974118\n",
      "Time take by the epoch 47 is 0:01:16.680940\n",
      "The Epoch number 48\n",
      "  batch 1000 loss: 1.574180635392666\n",
      "  batch 2000 loss: 3.1694490374326705\n",
      "  batch 3000 loss: 4.753591920733452\n",
      "  batch 4000 loss: 6.337715365707874\n",
      "  batch 5000 loss: 7.910328935056925\n",
      "  batch 6000 loss: 9.476828780263663\n",
      "  batch 7000 loss: 11.047142736405133\n",
      "  batch 8000 loss: 12.641973332375288\n",
      "  batch 9000 loss: 14.226716759592295\n",
      "  batch 10000 loss: 15.820577206343412\n",
      "  batch 11000 loss: 17.381019298404457\n",
      "  batch 12000 loss: 18.95902323883772\n",
      "  batch 13000 loss: 20.580260673820973\n",
      "  batch 14000 loss: 22.172730980813505\n",
      "  batch 15000 loss: 23.74413557726145\n",
      "  batch 16000 loss: 25.313035995841027\n",
      "  batch 17000 loss: 26.90993643653393\n",
      "Validation loss for the epoch 1.6361695388782684\n",
      "Time take by the epoch 48 is 0:01:16.634568\n",
      "The Epoch number 49\n",
      "  batch 1000 loss: 1.5816601297259332\n",
      "  batch 2000 loss: 3.173601801633835\n",
      "  batch 3000 loss: 4.756292121827602\n",
      "  batch 4000 loss: 6.328148328185081\n",
      "  batch 5000 loss: 7.913878071546555\n",
      "  batch 6000 loss: 9.50716955935955\n",
      "  batch 7000 loss: 11.066323937118053\n",
      "  batch 8000 loss: 12.644776926755906\n",
      "  batch 9000 loss: 14.215241249203682\n",
      "  batch 10000 loss: 15.808021984815598\n",
      "  batch 11000 loss: 17.39595814716816\n",
      "  batch 12000 loss: 18.973683321803808\n",
      "  batch 13000 loss: 20.53694587805867\n",
      "  batch 14000 loss: 22.131396485120057\n",
      "  batch 15000 loss: 23.736953323870896\n",
      "  batch 16000 loss: 25.306139160841703\n",
      "  batch 17000 loss: 26.90232152095437\n",
      "Validation loss for the epoch 1.6358377436612817\n",
      "Time take by the epoch 49 is 0:01:16.645414\n",
      "The Epoch number 50\n",
      "  batch 1000 loss: 1.5742191685438156\n",
      "  batch 2000 loss: 3.131590742915869\n",
      "  batch 3000 loss: 4.725899035304785\n",
      "  batch 4000 loss: 6.299000390976667\n",
      "  batch 5000 loss: 7.874933125466108\n",
      "  batch 6000 loss: 9.474670652359723\n",
      "  batch 7000 loss: 11.06101591166854\n",
      "  batch 8000 loss: 12.646686588257552\n",
      "  batch 9000 loss: 14.231045548409224\n",
      "  batch 10000 loss: 15.801798928707838\n",
      "  batch 11000 loss: 17.384072575002907\n",
      "  batch 12000 loss: 18.96019931420684\n",
      "  batch 13000 loss: 20.53238495913148\n",
      "  batch 14000 loss: 22.11060363844037\n",
      "  batch 15000 loss: 23.688861825555563\n",
      "  batch 16000 loss: 25.285094044059516\n",
      "  batch 17000 loss: 26.876634293347596\n",
      "Validation loss for the epoch 1.6340447153225903\n",
      "Time take by the epoch 50 is 0:01:16.613996\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_vloss = 1000000\n",
    "\n",
    "list_of_train_loss = []\n",
    "list_of_val_loss = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    print(f\"The Epoch number {epoch + 1}\")\n",
    "    \n",
    "    ############ Training Mode ##############################3\n",
    "    \n",
    "    #setting the model to traning mode. Now, certain layers such as batchNorm and dropout behave differently in\n",
    "    #in training and evaluation mode, hence this call helps to acheive it.\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss_for_batch = training_for_one_epoch(train_loader,epoch)\n",
    "    list_of_train_loss.append(avg_loss_for_batch)\n",
    "    \n",
    "    ################ Evaluation Mode ####################\n",
    "    #turns off dropout, batchnorm, adjusts the calculations\n",
    "    \n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    avg_val_loss = validate(test_loader)\n",
    "    list_of_val_loss.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Validation loss for the epoch\",avg_val_loss)\n",
    "    \n",
    "    \n",
    "    #Saving the best model!\n",
    "    if avg_val_loss<best_vloss:\n",
    "        #clearly our model is performing better for test set now! \n",
    "        # this might be our best performance\n",
    "        \n",
    "        best_vloss = avg_val_loss\n",
    "        model_path = \"model_{}\".format(epoch+1)\n",
    "        \n",
    "        # if you are only going to use the saved model for inference save the state_dict() this does not save parameters,buffers,etc and thus saves space.\n",
    "        #hoewever if you are going to use the model for fine tuning , do torch.save(model)\n",
    "        torch.save(model.state_dict(),model_path)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(f\"Time take by the epoch {epoch + 1} is {end_time-start_time}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90918340",
   "metadata": {
    "papermill": {
     "duration": 0.053122,
     "end_time": "2023-05-04T09:53:04.113765",
     "exception": false,
     "start_time": "2023-05-04T09:53:04.060643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plot the train and val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cef9a4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-04T09:53:04.220146Z",
     "iopub.status.busy": "2023-05-04T09:53:04.219794Z",
     "iopub.status.idle": "2023-05-04T09:53:04.550125Z",
     "shell.execute_reply": "2023-05-04T09:53:04.549169Z"
    },
    "papermill": {
     "duration": 0.385968,
     "end_time": "2023-05-04T09:53:04.552201",
     "exception": false,
     "start_time": "2023-05-04T09:53:04.166233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABcEUlEQVR4nO3dd3gU1f4G8HfTNptk0zsJSYDQIRQpASkCgihIFzBK02shIOjFe+V6pXkV+88OigoiIAgaRKSFEkQMvbdQEwKkkLbpdc/vj3GXLCmEZHcn2byf55lnZqfsfnei7us5Z2YUQggBIiIiIgthJXcBRERERMbEcENEREQWheGGiIiILArDDREREVkUhhsiIiKyKAw3REREZFEYboiIiMiiMNwQERGRRWG4ISIiIovCcEMkkylTpiA4OFjuMmQTHx8PhUKBFStW6NctWLAACoWiRscrFAosWLDAqDX1798f/fv3N+p71kRMTAwUCgViYmLM/tnG0NDrJ8vDcEN0F4VCUaOpMf2H/PHHH4eDgwNycnKq3CciIgJ2dnZIT083Y2X379y5c1iwYAHi4+PlLoUAbNmyxeghlchG7gKI6psffvjB4PXKlSsRHR1dYX2bNm3q9DnLli2DVqut03uYS0REBH777TdERUVh0qRJFbbn5+fj119/xSOPPAIPD49af85///tfvPbaa3Up9Z7OnTuHhQsXon///hVaznbs2GHSz6aKtmzZgi+++IIBh4yK4YboLk899ZTB6wMHDiA6OrrC+rvl5+fDwcGhxp9ja2tbq/rk8Pjjj0OtVmPNmjWVhptff/0VeXl5iIiIqNPn2NjYwMZGvv8s2dnZyfbZRGQ87JYiqoX+/fujffv2OHr0KPr27QsHBwf85z//ASD90D/22GPw9/eHUqlE8+bN8eabb6KsrMzgPe4ec6Mbg/LBBx/g66+/RvPmzaFUKtGtWzccPny42nqOHDkChUKB77//vsK27du3Q6FQYPPmzQCAnJwczJ49G8HBwVAqlfD29sbDDz+MY8eOVfn+KpUKo0ePxq5du5Camlph+5o1a6BWq/H4448jIyMDc+bMQYcOHeDk5ARnZ2cMHToUJ0+erPY7AJWPuSkqKsLLL78MLy8v/WfcuHGjwrEJCQmYPn06WrVqBZVKBQ8PD4wbN86g+2nFihUYN24cAOChhx6q0MVY2Zib1NRUPPPMM/Dx8YG9vT3CwsIqnOe6/O2qs379enTt2hUqlQqenp546qmncPPmTYN9kpOTMXXqVAQEBECpVMLPzw8jRoww+N5HjhzBkCFD4OnpCZVKhZCQEEybNu2enx8cHIxhw4Zhx44d6NSpE+zt7dG2bVv88ssvRql/ypQp+OKLLwAYdgcT1RVbbohqKT09HUOHDsWECRPw1FNPwcfHB4D0A+rk5IRXXnkFTk5O2L17N+bNm4fs7Gy8//7793zfNWvWICcnB88//zwUCgXee+89jB49GlevXq2yteeBBx5As2bN8NNPP2Hy5MkG29atWwc3NzcMGTIEAPDCCy9gw4YNmDFjBtq2bYv09HT8+eefOH/+PLp06VJlXREREfj+++/x008/YcaMGfr1GRkZ2L59OyZOnAiVSoWzZ89i48aNGDduHEJCQpCSkoKvvvoK/fr1w7lz5+Dv73/Pc1Des88+i1WrVuHJJ59Er169sHv3bjz22GMV9jt8+DD++usvTJgwAQEBAYiPj8eSJUvQv39/nDt3Dg4ODujbty9eeuklfPrpp/jPf/6j71qsqouxoKAA/fv3x+XLlzFjxgyEhIRg/fr1mDJlCrKysjBr1iyD/Wvzt6vKihUrMHXqVHTr1g2LFy9GSkoKPvnkE+zfvx/Hjx+Hq6srAGDMmDE4e/YsZs6cieDgYKSmpiI6OhrXr1/Xvx48eDC8vLzw2muvwdXVFfHx8TUOKJcuXcL48ePxwgsvYPLkyVi+fDnGjRuHbdu24eGHH65T/c8//zxu3bpVabcvUZ0IIqpWZGSkuPtflX79+gkAYunSpRX2z8/Pr7Du+eefFw4ODqKwsFC/bvLkySIoKEj/+tq1awKA8PDwEBkZGfr1v/76qwAgfvvtt2rrnDt3rrC1tTU4tqioSLi6uopp06bp17m4uIjIyMhq36sypaWlws/PT4SHhxusX7p0qQAgtm/fLoQQorCwUJSVlRnsc+3aNaFUKsWiRYsqfN/ly5fr182fP9/gXJ84cUIAENOnTzd4vyeffFIAEPPnz9evq+y8x8bGCgBi5cqV+nXr168XAMSePXsq7N+vXz/Rr18//euPP/5YABCrVq3SrysuLhbh4eHCyclJZGdnG3yX2v7t9uzZY1BTcXGx8Pb2Fu3btxcFBQX6/TZv3iwAiHnz5gkhhMjMzBQAxPvvv1/le0dFRQkA4vDhw9XWUJmgoCABQPz888/6dRqNRvj5+YnOnTvXuX4hKv/3i6iu2C1FVEtKpRJTp06tsF6lUumXc3JykJaWhj59+iA/Px8XLly45/uOHz8ebm5u+td9+vQBAFy9evWex5WUlBj8H/mOHTuQlZWF8ePH69e5urri4MGDuHXr1j1rKc/a2hoTJkxAbGysQZfHmjVr4OPjg4EDBwKQzouVlfSflrKyMqSnp8PJyQmtWrWqtuurMlu2bAEAvPTSSwbrZ8+eXWHf8ue9pKQE6enpaNGiBVxdXe/7c8t/vq+vLyZOnKhfZ2tri5deegm5ubnYu3evwf61/dvd7ciRI0hNTcX06dNhb2+vX//YY4+hdevW+P333wFI39nOzg4xMTHIzMys9L10LTybN29GSUnJfdUBAP7+/hg1apT+tbOzMyZNmoTjx48jOTm5TvUTmQrDDVEtNWnSpNIBqGfPnsWoUaPg4uICZ2dneHl56QcjazSae75v06ZNDV7rfiyr+vHSCQsLQ+vWrbFu3Tr9unXr1sHT0xMDBgzQr3vvvfdw5swZBAYGonv37liwYEGNf3x1A4bXrFkDALhx4wb27duHCRMmwNraGgCg1Wrxf//3fwgNDYVSqYSnpye8vLxw6tSpGn3/8hISEmBlZYXmzZsbrG/VqlWFfQsKCjBv3jwEBgYafG5WVtZ9f275zw8NDdWHNR1dN1ZCQoLB+tr+7Sr7XKDy79m6dWv9dqVSiXfffRdbt26Fj48P+vbti/fee88gdPTr1w9jxozBwoUL4enpiREjRmD58uUoKiqqUS0tWrSoMA6mZcuWAFDl5fQ1rZ/IVBhuiGqpfEuBTlZWFvr164eTJ09i0aJF+O233xAdHY13330XAGp06bcuJNxNCHHPY8ePH489e/YgLS0NRUVF2LRpE8aMGWNwBdITTzyBq1ev4rPPPoO/vz/ef/99tGvXDlu3br3n+3ft2hWtW7fGjz/+CAD48ccfIYQwuErq7bffxiuvvIK+ffti1apV2L59O6Kjo9GuXTuTXvo+c+ZMvPXWW3jiiSfw008/YceOHYiOjoaHh4fZLrmvy9+utmbPno2LFy9i8eLFsLe3xxtvvIE2bdrg+PHjAKSBuhs2bEBsbCxmzJiBmzdvYtq0aejatStyc3NNVheRnBhuiIwoJiYG6enpWLFiBWbNmoVhw4Zh0KBBBl0VpjR+/HiUlpbi559/xtatW5GdnY0JEyZU2M/Pzw/Tp0/Hxo0bce3aNXh4eOCtt96q0WdERETgzJkzOHXqFNasWYPQ0FB069ZNv33Dhg146KGH8O2332LChAkYPHgwBg0ahKysrPv+PkFBQdBqtbhy5YrB+ri4uAr7btiwAZMnT8aHH36IsWPH4uGHH8aDDz5Y4XPv52qcoKAgXLp0qUI40nUvBgUF1fi97ofufSv7nnFxcRU+t3nz5vjnP/+JHTt24MyZMyguLsaHH35osE/Pnj3x1ltv4ciRI1i9ejXOnj2LtWvX3rOWy5cvVwhnFy9eBIAq77B9P/Xz6igyBYYbIiPS/Z97+R+D4uJifPnll2b5/DZt2qBDhw5Yt24d1q1bBz8/P/Tt21e/vaysrEIXjbe3N/z9/WvcTaFrpZk3bx5OnDhR4d421tbWFX4M169fX+ES5poYOnQoAODTTz81WP/xxx9X2Leyz/3ss88qXILv6OgIADUKW48++iiSk5MNuvpKS0vx2WefwcnJCf369avJ17hvDzzwALy9vbF06VKDv8vWrVtx/vx5/dVi+fn5KCwsNDi2efPmUKvV+uMyMzMrnJdOnToBQI3+5rdu3UJUVJT+dXZ2NlauXIlOnTrB19e3TvUD9/f3IKopXgpOZES9evWCm5sbJk+ejJdeegkKhQI//PCDSbsl7jZ+/HjMmzcP9vb2eOaZZwzGi+Tk5CAgIABjx45FWFgYnJycsHPnThw+fLjC/+lXJSQkBL169cKvv/4KABXCzbBhw7Bo0SJMnToVvXr1wunTp7F69Wo0a9bsvr9Lp06dMHHiRHz55ZfQaDTo1asXdu3ahcuXL1fYd9iwYfjhhx/g4uKCtm3bIjY2Fjt37qxwx+ROnTrB2toa7777LjQaDZRKJQYMGABvb+8K7/ncc8/hq6++wpQpU3D06FEEBwdjw4YN2L9/Pz7++GOo1er7/k41YWtri3fffRdTp05Fv379MHHiRP2l1MHBwXj55ZcBSC0oAwcOxBNPPIG2bdvCxsYGUVFRSElJ0bfYff/99/jyyy8xatQoNG/eHDk5OVi2bBmcnZ3x6KOP3rOWli1b4plnnsHhw4fh4+OD7777DikpKVi+fHmd6wekrk5AGjQ+ZMgQ/cB1ojqR7TotogaiqkvB27VrV+n++/fvFz179hQqlUr4+/uLf/3rX2L79u0VLj+u6lLwyi7rxV2XPVfn0qVLAoAAIP7880+DbUVFReLVV18VYWFhQq1WC0dHRxEWFia+/PLLGr23zhdffCEAiO7du1fYVlhYKP75z38KPz8/oVKpRO/evUVsbGyFy6xrcim4EEIUFBSIl156SXh4eAhHR0cxfPhwkZiYWOGcZGZmiqlTpwpPT0/h5OQkhgwZIi5cuCCCgoLE5MmTDd5z2bJlolmzZsLa2trg73J3jUIIkZKSon9fOzs70aFDB4Oay3+X2v7t7r6UWmfdunWic+fOQqlUCnd3dxERESFu3Lih356WliYiIyNF69athaOjo3BxcRE9evQQP/30k36fY8eOiYkTJ4qmTZsKpVIpvL29xbBhw8SRI0eqrUkI6VLwxx57TGzfvl107NhRKJVK0bp1a7F+/Xqj1C+EdIuBmTNnCi8vL6FQKHhZOBmFQggz/i8lERE1GMHBwWjfvr3+7tZEDQXH3BAREZFFYbghIiIii8JwQ0RERBaFY26IiIjIorDlhoiIiCwKww0RERFZlEZ3Ez+tVotbt25BrVbztt9EREQNhBACOTk58Pf3r/Aw27s1unBz69YtBAYGyl0GERER1UJiYiICAgKq3afRhRvd7dITExPh7OwsczVERERUE9nZ2QgMDKzRY08aXbjRdUU5Ozsz3BARETUwNRlSwgHFREREZFEYboiIiMiiMNwQERGRRWl0Y26IiMgylZWVoaSkRO4yqA7s7OzueZl3TTDcEBFRgyaEQHJyMrKysuQuherIysoKISEhsLOzq9P7MNwQEVGDpgs23t7ecHBw4A1aGyjdTXaTkpLQtGnTOv0dGW6IiKjBKisr0wcbDw8PucuhOvLy8sKtW7dQWloKW1vbWr8PBxQTEVGDpRtj4+DgIHMlZAy67qiysrI6vQ/DDRERNXjsirIMxvo7MtwQERGRRWG4ISIiauCCg4Px8ccfG+W9YmJioFAoGvTVZxxQTEREJIP+/fujU6dORgklhw8fhqOjY92LshBsuTGSMm0ZknOTcTH9otylEBGRBRBCoLS0tEb7enl5cVB1OQw3RnJdcx1+H/ohbGmY3KUQEVE9N2XKFOzduxeffPIJFAoFFAoFVqxYAYVCga1bt6Jr165QKpX4888/ceXKFYwYMQI+Pj5wcnJCt27dsHPnToP3u7tbSqFQ4JtvvsGoUaPg4OCA0NBQbNq0qdb1/vzzz2jXrh2USiWCg4Px4YcfGmz/8ssvERoaCnt7e/j4+GDs2LH6bRs2bECHDh2gUqng4eGBQYMGIS8vr9a11AS7pYzEXeUOACgsLURhaSHsbexlroiIqHESQiC/JF+Wz3awrdlNBD/55BNcvHgR7du3x6JFiwAAZ8+eBQC89tpr+OCDD9CsWTO4ubkhMTERjz76KN566y0olUqsXLkSw4cPR1xcHJo2bVrlZyxcuBDvvfce3n//fXz22WeIiIhAQkIC3N3d7+s7HT16FE888QQWLFiA8ePH46+//sL06dPh4eGBKVOm4MiRI3jppZfwww8/oFevXsjIyMC+ffsAAElJSZg4cSLee+89jBo1Cjk5Odi3bx+EEPdVw/1iuDEStVINK4UVtEKLzIJM+Kn95C6JiKhRyi/Jh9NiJ1k+O3duLhzt7j32xcXFBXZ2dnBwcICvry8A4MKFCwCARYsW4eGHH9bv6+7ujrCwO70Cb775JqKiorBp0ybMmDGjys+YMmUKJk6cCAB4++238emnn+LQoUN45JFH7us7ffTRRxg4cCDeeOMNAEDLli1x7tw5vP/++5gyZQquX78OR0dHDBs2DGq1GkFBQejcuTMAKdyUlpZi9OjRCAoKAgB06NDhvj6/NtgtZSRWCiu42rsCADILM+UthoiIGqwHHnjA4HVubi7mzJmDNm3awNXVFU5OTjh//jyuX79e7ft07NhRv+zo6AhnZ2ekpqbedz3nz59H7969Ddb17t0bly5dQllZGR5++GEEBQWhWbNmePrpp7F69Wrk50stZ2FhYRg4cCA6dOiAcePGYdmyZcjMNP1vJFtujMjN3g0ZBRnIKMiQuxQiokbLwdYBuXNzZfvsurr7qqc5c+YgOjoaH3zwAVq0aAGVSoWxY8eiuLi42ve5+/EFCoUCWq22zvXdTa1W49ixY4iJicGOHTswb948LFiwAIcPH4arqyuio6Px119/YceOHfjss8/w+uuv4+DBgwgJCTF6LToMN0bkpnIDMoHMArbcEBHJRaFQ1KhrSG52dnY1eszA/v37MWXKFIwaNQqA1JITHx9v4uruaNOmDfbv31+hppYtW8La2hoAYGNjg0GDBmHQoEGYP38+XF1dsXv3bowePRoKhQK9e/dG7969MW/ePAQFBSEqKgqvvPKKyWpmuDEi3aBidksREdG9BAcH4+DBg4iPj4eTk1OVrSqhoaH45ZdfMHz4cCgUCrzxxhsmaYGpyj//+U9069YNb775JsaPH4/Y2Fh8/vnn+PLLLwEAmzdvxtWrV9G3b1+4ublhy5Yt0Gq1aNWqFQ4ePIhdu3Zh8ODB8Pb2xsGDB3H79m20adPGpDVzzI0Rudm7AWDLDRER3ducOXNgbW2Ntm3bwsvLq8oxNB999BHc3NzQq1cvDB8+HEOGDEGXLl3MVmeXLl3w008/Ye3atWjfvj3mzZuHRYsWYcqUKQAAV1dX/PLLLxgwYADatGmDpUuX4scff0S7du3g7OyMP/74A48++ihatmyJ//73v/jwww8xdOhQk9asEKa+Hqueyc7OhouLCzQaDZydnY363i9ufhFLjy7F/H7zsaD/AqO+NxERVVRYWIhr164hJCQE9va8BUdDV93f835+v9lyY0RuKrbcEBERyY3hxoh03VIZhbxaioiI6qcXXngBTk5OlU4vvPCC3OUZBQcUG5F+QDFbboiIqJ5atGgR5syZU+k2Yw/XkAvDjRHpu6V4tRQREdVT3t7e8Pb2lrsMk2K3lBHxaikiIiL5MdwYEVtuiIiI5MdwY0T6AcUFGSZ/4ikRERFVjuHGiHQDiovLilFQWiBzNURERI2TrOFm8eLF6NatG9RqNby9vTFy5EjExcXd87isrCxERkbCz88PSqUSLVu2xJYtW8xQcfWc7JxgrZCes8FxN0RERPKQNdzs3bsXkZGROHDgAKKjo1FSUoLBgwcjLy+vymOKi4vx8MMPIz4+Hhs2bEBcXByWLVuGJk2amLHyyikUCo67ISIiswgODsbHH39co30VCgU2btxo0nrqE1kvBd+2bZvB6xUrVsDb2xtHjx5F3759Kz3mu+++Q0ZGBv766y/949yDg4NNXWqNudm7IS0/jS03REREMqlXY240Gg0AwN3dvcp9Nm3ahPDwcERGRsLHxwft27fH22+/XeVj44uKipCdnW0wmZKu5SajgHcpJiIikkO9CTdarRazZ89G79690b59+yr3u3r1KjZs2ICysjJs2bIFb7zxBj788EP873//q3T/xYsXw8XFRT8FBgaa6isAKHevG3ZLERFRFb7++mv4+/tDq9UarB8xYgSmTZuGK1euYMSIEfDx8YGTkxO6deuGnTt3Gu3zT58+jQEDBkClUsHDwwPPPfcccnNz9dtjYmLQvXt3ODo6wtXVFb1790ZCQgIA4OTJk3jooYegVqvh7OyMrl274siRI0arzRjqTbiJjIzEmTNnsHbt2mr302q18Pb2xtdff42uXbti/PjxeP3117F06dJK9587dy40Go1+SkxMNEX5enwEAxGRvIQA8vLkmWp6F5Bx48YhPT0de/bs0a/LyMjAtm3bEBERgdzcXDz66KPYtWsXjh8/jkceeQTDhw/H9evX63x+8vLyMGTIELi5ueHw4cNYv349du7ciRkzZgAASktLMXLkSPTr1w+nTp1CbGwsnnvuOSgUCgBAREQEAgICcPjwYRw9ehSvvfaafphIfVEvHr8wY8YMbN68GX/88QcCAgKq3dfPzw+2trawtrbWr2vTpg2Sk5NRXFwMOzs7g/2VSiWUSqVJ6q4MW26IiOSVnw84Ocnz2bm5gKPjvfdzc3PD0KFDsWbNGgwcOBAAsGHDBnh6euKhhx6ClZUVwsLC9Pu/+eabiIqKwqZNm/QhpLbWrFmDwsJCrFy5Eo5/F/v5559j+PDhePfdd2FrawuNRoNhw4ahefPmAKTfWZ3r16/j1VdfRevWrQEAoaGhdarHFGRtuRFCYMaMGYiKisLu3bsREhJyz2N69+6Ny5cvGzTlXbx4EX5+fhWCjRz0V0ux5YaIiKoRERGBn3/+GUVFRQCA1atXY8KECbCyskJubi7mzJmDNm3awNXVFU5OTjh//rxRWm7Onz+PsLAwfbABpN9WrVaLuLg4uLu7Y8qUKRgyZAiGDx+OTz75BElJSfp9X3nlFTz77LMYNGgQ3nnnHVy5cqXONRmbrOEmMjISq1atwpo1a6BWq5GcnIzk5GQUFNy5Ad6kSZMwd+5c/esXX3wRGRkZmDVrFi5evIjff/8db7/9NiIjI+X4ChXo71JcyAHFRERycHCQWlDkmBwcal7n8OHDIYTA77//jsTEROzbtw8REREAgDlz5iAqKgpvv/029u3bhxMnTqBDhw4oLi420VkztHz5csTGxqJXr15Yt24dWrZsiQMHDgAAFixYgLNnz+Kxxx7D7t270bZtW0RFRZmlrpqStVtqyZIlAID+/fsbrF++fDmmTJkCQGr+srK6k8ECAwOxfft2vPzyy+jYsSOaNGmCWbNm4d///re5yq4WW26IiOSlUNSsa0hu9vb2GD16NFavXo3Lly+jVatW6NKlCwBg//79mDJlCkaNGgUAyM3NRXx8vFE+t02bNlixYgXy8vL0rTf79++HlZUVWrVqpd+vc+fO6Ny5M+bOnYvw8HCsWbMGPXv2BAC0bNkSLVu2xMsvv4yJEydi+fLl+lrrA1nDTU2evxQTE1NhXXh4uD5B1jf6AcUcc0NERPcQERGBYcOG4ezZs3jqqaf060NDQ/HLL79g+PDhUCgUeOONNypcWVWXz5w/fz4mT56MBQsW4Pbt25g5cyaefvpp+Pj44Nq1a/j666/x+OOPw9/fH3Fxcbh06RImTZqEgoICvPrqqxg7dixCQkJw48YNHD58GGPGjDFKbcZSLwYUWxL9gGK23BAR0T0MGDAA7u7uiIuLw5NPPqlf/9FHH2HatGno1asXPD098e9//9to92lzcHDA9u3bMWvWLHTr1g0ODg4YM2YMPvroI/32Cxcu4Pvvv0d6ejr8/PwQGRmJ559/HqWlpUhPT8ekSZOQkpICT09PjB49GgsXLjRKbcaiEI3s8dXZ2dlwcXGBRqOBs7Oz0d//VMophC0Ng7ejN1LmpBj9/YmI6I7CwkJcu3YNISEhsLe3l7scqqPq/p738/tdb+5zYyn0A4oLMmrU7UZERETGxXBjZLoBxaXaUuSVVP0AUCIiImNYvXo1nJycKp3atWsnd3my4JgbI3O0dYStlS1KtCXILMiEk51Md5IiIqJG4fHHH0ePHj0q3Vbf7hxsLgw3RqZQKOCmckNqXioyCzMR6GLaZ1kREVHjplaroVar5S6jXmG3lAnwiikiIiL5MNyYgG7cTUYB71JMRGQOxroHDMnLWBfisFvKBPjwTCIi87Czs4OVlRVu3boFLy8v2NnZ6Z9eTQ2LEAK3b9+GQqGo81ghhhsT0N+lmN1SREQmZWVlhZCQECQlJeHWrVtyl0N1pFAoEBAQAGtr6zq9D8ONCbDlhojIfOzs7NC0aVOUlpairKxM7nKoDmxtbescbACGG5PgwzOJiMxL15XRWC99JkMcUGwC+rsUF3JAMRERkbkx3JgAW26IiIjkw3BjAhxzQ0REJB+GGxPg1VJERETyYbgxAX23FFtuiIiIzI7hxgTKP37BWHdbJCIiopphuDEBXctNmShDTnGOzNUQERE1Lgw3JqCyUcHO2g4Ax90QERGZG8ONCSgUijuDijnuhoiIyKwYbkyk/LgbIiIiMh+GGxPhFVNERETyYLgxEf0jGAr4CAYiIiJzYrgxET6CgYiISB4MNybibs8BxURERHJguDERttwQERHJg+HGRPjwTCIiInkw3JiIruWGA4qJiIjMi+HGRNhyQ0REJA+GGxPhmBsiIiJ5MNyYCB+/QEREJA+GGxPRdUtlFWZBK7QyV0NERNR4MNyYiK5bSiu0yC7KlrkaIiKixoPhxkTsbexhb2MPgONuiIiIzInhxoR4xRQREZH5MdyYkH5QMVtuiIiIzIbhxoT0l4Oz5YaIiMhsGG5MSNctxbsUExERmQ/DjQnxRn5ERETmx3BjQhxQTEREZH4MNybEAcVERETmx3BjQmy5ISIiMj+GGxPSjbnhgGIiIiLzYbgxIbbcEBERmR/DjQnxaikiIiLzY7gxIf2AYrbcEBERmQ3DjQnpuqU0hRqUactkroaIiKhxYLgxIV23lICApkgjczVERESNA8ONCdlZ28HB1gEAx90QERGZC8ONifGKKSIiIvOSNdwsXrwY3bp1g1qthre3N0aOHIm4uLgaH7927VooFAqMHDnSdEXWEa+YIiIiMi9Zw83evXsRGRmJAwcOIDo6GiUlJRg8eDDy8vLueWx8fDzmzJmDPn36mKHS2uMVU0REROZlI+eHb9u2zeD1ihUr4O3tjaNHj6Jv375VHldWVoaIiAgsXLgQ+/btQ1ZWlokrrT1dtxTvUkxERGQe9WrMjUYjXVHk7u5e7X6LFi2Ct7c3nnnmmXu+Z1FREbKzsw0mc2K3FBERkXnVm3Cj1Woxe/Zs9O7dG+3bt69yvz///BPffvstli1bVqP3Xbx4MVxcXPRTYGCgsUquEQ4oJiIiMq96E24iIyNx5swZrF27tsp9cnJy8PTTT2PZsmXw9PSs0fvOnTsXGo1GPyUmJhqr5BrRhxu23BAREZmFrGNudGbMmIHNmzfjjz/+QEBAQJX7XblyBfHx8Rg+fLh+nVarBQDY2NggLi4OzZs3NzhGqVRCqVSapvAa4IBiIiIi85I13AghMHPmTERFRSEmJgYhISHV7t+6dWucPn3aYN1///tf5OTk4JNPPjF7l1NN6MfcMNwQERGZhazhJjIyEmvWrMGvv/4KtVqN5ORkAICLiwtUKhUAYNKkSWjSpAkWL14Me3v7CuNxXF1dAaDacTpy4tVSRERE5iVruFmyZAkAoH///gbrly9fjilTpgAArl+/DiurejM06L7xaikiIiLzkr1b6l5iYmKq3b5ixQrjFGMivFqKiIjIvBpuk0gDoRtQnF2UjTJtmczVEBERWT6GGxNztXfVL2cVZslWBxERUWPBcGNitta2cLJzAsBBxURERObAcGMGHHdDRERkPgw3ZsArpoiIiMyH4cYM2HJDRERkPgw3ZqB/BANbboiIiEyO4cYMeJdiIiIi82G4MQM+X4qIiMh8GG7MQD/mht1SREREJsdwYwZsuSEiIjIfhhsz0A8oZrghIiIyOYYbM+CAYiIiIvNhuDED3sSPiIjIfBhuzIA38SMiIjIfhhsz0LXc5BbnoqSsROZqiIiILBvDjRm42rvql7MKs2Srg4iIqDFguDEDGysbOCudAXBQMRERkakx3JgJx90QERGZB8ONmfCKKSIiIvNguDETttwQERGZB8ONmejvUsyWGyIiIpNiuDET3qWYiIjIPBhuzIQPzyQiIjIPhhsz4ZgbIiIi82C4MRNeLUVERGQeDDdmwpYbIiIi82C4MRPd1VIcUExERGRaDDdGdOUKsHVr5dvYLUVERGQeNnIXYCn27wf69AE8PIAbNwCl0nA7u6WIiIjMgy03RtKjB9CkCZCWBqxfX3G7ruUmvyQfxWXFZq6OiIio8WC4MRIbG+D556XlL7+suN1F6aJfZtcUERGR6TDcGNGzz0ohJzYWOH7ccJu1lTVc7V0BsGuKiIjIlBhujMjXFxgzRlpesqTidj6CgYiIyPQYbowsMlKar14NZGUZbuMVU0RERKbHcGNkDz4ItG8P5OcD339vuI1XTBEREZkew42RKRTA9OnS8pdfAkLc2caWGyIiItNjuDGBp54CnJyAixeB3bvvrHe3l+5SzJYbIiIi02G4MQG1Gpg0SVouf1m4ruWGA4qJiIhMh+HGRHRdU7/+Kt2xGOCYGyIiInNguDGRdu2Afv2AsjLg66+ldRxzQ0REZHoMNyaka71ZtgwoLmbLDRERkTkw3JjQyJHSjf2Sk4GNG9lyQ0REZA4MNyZkZwf84x/S8hdfAO4q6WopDigmIiIyHYYbE3vuOcDaGvjjDyD1qjcAdksRERGZEsONiQUEACNGSMsbfvACABSWFqKwtFDGqoiIiCwXw40Z6AYWr1ttBxSpAXDcDRERkakw3JjBgAFAq1ZAbq4CDuelQTi382/LXBUREZFlYrgxA4UCePHFv5cPzwAE8OuFX+UtioiIyEIx3JjJ5MmAgwOQdzMESOiDb49/C63Qyl0WERGRxWG4MRNXVyAiQlq2PTYLCZoE7Lq6S9aaiIiILJGs4Wbx4sXo1q0b1Go1vL29MXLkSMTFxVV7zLJly9CnTx+4ubnBzc0NgwYNwqFDh8xUcd3oBhaXnR0J5Hrjm+PfyFoPERGRJZI13OzduxeRkZE4cOAAoqOjUVJSgsGDByMvL6/KY2JiYjBx4kTs2bMHsbGxCAwMxODBg3Hz5k0zVl47nToB3bsD2jJr4MwERJ2PQlp+mtxlERERWRSFEELIXYTO7du34e3tjb1796Jv3741OqasrAxubm74/PPPMWnSpHvun52dDRcXF2g0Gjg7O9e15Pv26afArFmAQ/AZ5E/pgI8Gf4SXw182ex1EREQNyf38fterMTcajQYA4O7uXuNj8vPzUVJSUuUxRUVFyM7ONpjkNH48YGUF5Me3B9Jb4Jvj36Ae5UsiIqIGr96EG61Wi9mzZ6N3795o3759jY/797//DX9/fwwaNKjS7YsXL4aLi4t+CgwMNFbJteLjA+hKtTk7Cedun8PBmwdlrYmIiMiS1JtwExkZiTNnzmDt2rU1Puadd97B2rVrERUVBXt7+0r3mTt3LjQajX5KTEw0Vsm1prtqyiHuGUAA3xzjwGIiIiJjqRfhZsaMGdi8eTP27NmDgICAGh3zwQcf4J133sGOHTvQsWPHKvdTKpVwdnY2mOQ2ciRgbw9k3/QHkrpg7Zm1yCnKkbssIiIiiyBruBFCYMaMGYiKisLu3bsREhJSo+Pee+89vPnmm9i2bRseeOABE1dpfM7OwPDh0rLbpUjkleRh3dl18hZFRERkIWQNN5GRkVi1ahXWrFkDtVqN5ORkJCcno6CgQL/PpEmTMHfuXP3rd999F2+88Qa+++47BAcH64/Jzc2V4yvUmq5rquz0OEBrxa4pIiIiI5E13CxZsgQajQb9+/eHn5+fflq37k4rxvXr15GUlGRwTHFxMcaOHWtwzAcffCDHV6i1Rx6R7lqcnaaG1fUBOHjzIE6nnJa7LCIiogbPRs4Pr8kl0DExMQav4+PjTVOMmSmVwLhxwLJlQGDCv5EQvBPfHv8WHz/ysdylERERNWj1YkBxY/Xkk9I87Ug/oESJH079gMLSQnmLIiIiauAYbmTUty/QpAmQl2MLj5tPI6MgAxsvbJS7LCIiogaN4UZGVlbAxInSsvfV2QB4zxsiIqK6YriRma5r6srBtkChC3Zd24VrmdfkLYqIiKgBY7iRWadOQJs2QHGRAu3S/wsA+O74d/IWRURE1IDVKtwkJibixo0b+teHDh3C7Nmz8fXXXxutsMZCobjTemN95ikAwPITy1GqLZWxKiIiooarVuHmySefxJ49ewAAycnJePjhh3Ho0CG8/vrrWLRokVELbAx0427OHPSBW2kb3My5ie2Xt8tbFBERUQNVq3Bz5swZdO/eHQDw008/oX379vjrr7+wevVqrFixwpj1NQrNmwM9ewJarQId094CAHxznAOLiYiIaqNW4aakpARKpRIAsHPnTjz++OMAgNatWxvcTZhqTtc1lX5gCADgt7jfkJybLGNFREREDVOtwk27du2wdOlS7Nu3D9HR0XjkkUcAALdu3YKHh4dRC2wsnngCsLYGzpxwQJjtWJSJMqw/u17usoiIiBqcWoWbd999F1999RX69++PiRMnIiwsDACwadMmfXcV3R8fH2DQIGnZ68pLAIDoq9EyVkRERNQwKURNHvBUibKyMmRnZ8PNzU2/Lj4+Hg4ODvD29jZagcaWnZ0NFxcXaDQaODs7y12OgZUrgcmTgaBmhUh4WgW1Uo30f6XD1tpW7tKIiIhkdT+/37VquSkoKEBRUZE+2CQkJODjjz9GXFxcvQ429d2oUYC9PZBw1R4uGQOQU5yDQzcPyV0WERFRg1KrcDNixAisXLkSAJCVlYUePXrgww8/xMiRI7FkyRKjFtiYqNXA32Oz4XP1ZQDAzqs7ZayIiIio4alVuDl27Bj69OkDANiwYQN8fHyQkJCAlStX4tNPPzVqgY1NRIQ0Tzn4EKC14rgbIiKi+1SrcJOfnw+1Wg0A2LFjB0aPHg0rKyv07NkTCQkJRi2wsXnkEcDNDdCkOQLx/XDgxgFkF2XLXRYREVGDUatw06JFC2zcuBGJiYnYvn07Bg8eDABITU2td4N0Gxo7O2DYMGnZLWU0ykQZ9sbvlbcoIiKiBqRW4WbevHmYM2cOgoOD0b17d4SHhwOQWnE6d+5s1AIbo79PJ5xuPwSAl4QTERHdD5vaHDR27Fg8+OCDSEpK0t/jBgAGDhyIUaNGGa24xkp3q6DMy6GA4KBiIiKi+1GrcAMAvr6+8PX11T8dPCAggDfwM5IOHQClEsjNtoMiMxTnFedxI/sGApwD5C6NiIio3qtVt5RWq8WiRYvg4uKCoKAgBAUFwdXVFW+++Sa0Wq2xa2x07OwAXe9eSP54AMCuq7tkrIiIiKjhqFW4ef311/H555/jnXfewfHjx3H8+HG8/fbb+Oyzz/DGG28Yu8ZGSdcI5pEhPbeL426IiIhqplbdUt9//z2++eYb/dPAAaBjx45o0qQJpk+fjrfeestoBTZWunCTH98e6CiNuxFCQKFQyFsYERFRPVerlpuMjAy0bt26wvrWrVsjIyOjzkXRnXBz+awzVFbOSMlLwZnUM/IWRURE1ADUKtyEhYXh888/r7D+888/R8eOHetcFAEtWgCurkBRkQKdFE8D4FVTRERENVGrbqn33nsPjz32GHbu3Km/x01sbCwSExOxZcsWoxbYWCkUUuvNjh2Af/ZIwPELRF+NxsvhL8tdGhERUb1Wq5abfv364eLFixg1ahSysrKQlZWF0aNH4+zZs/jhhx+MXWOjpeuaKk3sAgDYm7AXxWXFMlZERERU/ymEEMJYb3by5El06dIFZWVlxnpLo8vOzoaLiws0Gk29f1TEb79JTwlv107g9hRfpOalImZyDPoF95O7NCIiIrO6n9/vWrXckHl06ybNz51ToJ+f9MApjrshIiKqHsNNPebrCzRtCggBNCsYB4D3uyEiIroXhpt6TjfuxvpWTwDA4VuHkVmQKWNFRERE9dt9XS01evToardnZWXVpRaqRPfuwIYNQNwpV7R+qDUupF1ATHwMRrXhA0qJiIgqc18tNy4uLtVOQUFBmDRpkqlqbZR0LTeHDgEPN3sYALumiIiIqnNfLTfLly83VR1Uha5dASsrIDER6OL0GIDPOKiYiIioGhxzU885OQFt20rL9ql9YK2wxqWMS0jISpC3MCIionqK4aYB6NFDmp857oAeAdILtt4QERFVjuGmAeC4GyIioppjuGkAdOHm8GFgQPAgAMCua7ugFVoZqyIiIqqfGG4agHbtAJUKyMoCPAp6wMnOCWn5aTiZfFLu0oiIiOodhpsGwNYW6CI9OxPHj9qif3B/ABx3Q0REVBmGmwaC426IiIhqhuGmgags3Oy7vg+FpYUyVkVERFT/MNw0ELpwc/w40My5NfzV/igsLcT+6/vlLYyIiKieYbhpIEJCAA8PoLgYOH1aoW+94bgbIiIiQww3DYRCYdg11TuwNwDgaNJRGasiIiKqfxhuGpDy4SbMNwwAcDKFl4MTERGVx3DTgJQPN+2928NKYYXUvFSk5KbIWxgREVE9wnDTgHTrJs0vXABK8h0Q6h4KgK03RERE5THcNCBeXtLAYiGAo0fLdU3xTsVERER6DDcNTPmuqY7eHQGw5YaIiKg8hpsGhoOKiYiIqidruFm8eDG6desGtVoNb29vjBw5EnFxcfc8bv369WjdujXs7e3RoUMHbNmyxQzV1g8G4cZHCjcX0i6gqLRIxqqIiIjqD1nDzd69exEZGYkDBw4gOjoaJSUlGDx4MPLy8qo85q+//sLEiRPxzDPP4Pjx4xg5ciRGjhyJM2fOmLFy+XTuDFhbAzdvAoqcALjZu6FUW4rzaeflLo2IiKheUAghhNxF6Ny+fRve3t7Yu3cv+vbtW+k+48ePR15eHjZv3qxf17NnT3Tq1AlLly6952dkZ2fDxcUFGo0Gzs7ORqvdnDp1Ak6eBKKigE80DyEmPgYrRqzA5E6T5S6NiIjIJO7n97tejbnRaDQAAHd39yr3iY2NxaBBgwzWDRkyBLGxsZXuX1RUhOzsbIOpodN1TR08yEHFREREd6s34Uar1WL27Nno3bs32rdvX+V+ycnJ8PHxMVjn4+OD5OTkSvdfvHgxXFxc9FNgYKBR65YDBxUTERFVrd6Em8jISJw5cwZr16416vvOnTsXGo1GPyUmJhr1/eWgCzeHDwMdvO7c66Ye9TASERHJxkbuAgBgxowZ2Lx5M/744w8EBARUu6+vry9SUgwfN5CSkgJfX99K91cqlVAqlUartT5o2xZwcABycgDbzPawVlgjvSAdSblJ8Ff7y10eERGRrGRtuRFCYMaMGYiKisLu3bsREhJyz2PCw8Oxa9cug3XR0dEIDw83VZn1jo0N0LWrtHzymBKtPFtJy7xTMRERkbzhJjIyEqtWrcKaNWugVquRnJyM5ORkFBQU6PeZNGkS5s6dq389a9YsbNu2DR9++CEuXLiABQsW4MiRI5gxY4YcX0E2Bncq9uGgYiIiIh1Zw82SJUug0WjQv39/+Pn56ad169bp97l+/TqSkpL0r3v16oU1a9bg66+/RlhYGDZs2ICNGzdWOwjZEulabo4fv3MzP4YbIiIimcfc1GQAbExMTIV148aNw7hx40xQUcPRubM0P3kS+I+nFG5OpZySsSIiIqL6od5cLUX3JzRUGlScnw8450nNOHFpcSgsLZS5MiIiInkx3DRQ1tZAmNRggxtxXvBQeaBMlOFs6ll5CyMiIpIZw00DpuuaOn5cwZv5ERER/Y3hpgG7E27KDSrm5eBERNTIMdw0YF26SPPjx4GO3n8PKk7loGIiImrcGG4asHbtpBv6ZWQAPmUPAOBjGIiIiBhuGjClUgo4AJB3vSVsrGyQWZiJG9k35C2MiIhIRgw3DZxu3M2ZU7Zo7dkaAAcVExFR48Zw08Dpws2xYxxUTEREBDDcNHiVXTHFQcVERNSYMdw0cPob+d0Agu3uDComIiJqrBhuGjhnZ+lRDACgTZKSzqWMS8gvyZexKiIiIvkw3FgAXddUwgV3eDt6Qyu0OJN6Rt6iiIiIZMJwYwEqHXfDJ4QTEVEjxXBjAXjFFBER0R0MNxZAF24uXQJaqqVnMvBeN0RE1Fgx3FgAb2/A3x8QArBL6wZA6pbiYxiIiKgxYrixELrWm8xrwbC1soWmSIMETYK8RREREcmA4cZC6J4QfvqkDdp6tQXAQcVERNQ4MdxYCIMrpnw5qJiIiBovhhsLoX+A5hmgrVsnABxUTEREjRPDjYUICgLc3ICSEsAluxcAhhsiImqcGG4shEIBdOokLRfdkMbcXMm4gtziXPmKIiIikgHDjQXRdU1dPqeGn5MfBAQfw0BERI0Ow40F4aBiIiIihhuLorsc/ORJoIPn3+GG426IiKiRYbixIK1aASoVkJsL+BT3BsBwQ0REjQ/DjQWxtgY6dpSWtbc6AZBu5KcVWvmKIiIiMjOGGwujG3eTeqUJ7KztkFuci/iseFlrIiIiMieGGwujCzcnT1ihnVc7aZmDiomIqBFhuLEw5a+Y6ujDQcVERNT4MNxYmA4dpLE3aWlAsBUHFRMRUePDcGNh7O2BNm2kZeXtngCAAzcOQAghY1VERETmw3BjgXT3u8m/3hr2NvZIzk3G+bTz8hZFRERkJgw3Fkg37ub0SRs82PRBAMDua7tlrIiIiMh8GG4sUPlBxQOCBwAAdl3bJWNFRERE5sNwY4F0Twe/fh14wG0IACAmPgZl2jL5iiIiIjIThhsL5OICNGsmLYukMDgrnZFVmIUTySdkrYuIiMgcGG4slK5r6tRJa/QP7g+AXVNERNQ4MNxYKF24OXbszrgbDiomIqLGgOHGQukuBz9+HBgQIoWbfdf3obisWMaqiIiITI/hxkLpWm7i4oAQx/bwcvBCfkk+Dt44KG9hREREJsZwY6F8faVJCOD0aYW+9YZdU0REZOkYbiyYwf1uQni/GyIiahwYbixY+XAzMGQgAOk5U3nFeTJWRUREZFoMNxZMN6h4zx4gyLkZmro0RYm2BH9e/1PewoiIiEyI4caCDRkCuLoCV64Av/2m0LfecNwNERFZMoYbC+bkBERGSsvvvgs8pLvfTTzDDRERWS6GGws3cyagVAIHDwJOyY8AAI7eOorMgkyZKyMiIjINhhsL5+MDTJkiLX/3uSdae7aGgMDehL2y1kVERGQqDDeNwD//CSgUwObNQBgiAAC7rvKScCIiskyyhps//vgDw4cPh7+/PxQKBTZu3HjPY1avXo2wsDA4ODjAz88P06ZNQ3p6uumLbcBCQ4HRo6XllJ1PA+C4GyIislyyhpu8vDyEhYXhiy++qNH++/fvx6RJk/DMM8/g7NmzWL9+PQ4dOoR//OMfJq604Xv1VWn+5+amgCYA526fQ3JusrxFERERmYCs4Wbo0KH43//+h1GjRtVo/9jYWAQHB+Oll15CSEgIHnzwQTz//PM4dOiQiStt+Hr0APr1A0pLFfA+/RYAXhJORESWqUGNuQkPD0diYiK2bNkCIQRSUlKwYcMGPProo1UeU1RUhOzsbIOpsfrXv6R55v4ngAIXhhsiIrJIDSrc9O7dG6tXr8b48eNhZ2cHX19fuLi4VNuttXjxYri4uOinwMBAM1ZcvwwdCrRvD5QU2ANHXuBzpoiIyCI1qHBz7tw5zJo1C/PmzcPRo0exbds2xMfH44UXXqjymLlz50Kj0einxMREM1ZcvygUd8be4OAsxN9OwrXMa7LWREREZGwNKtwsXrwYvXv3xquvvoqOHTtiyJAh+PLLL/Hdd98hKSmp0mOUSiWcnZ0NpsZswgQgIABArh9w6il2TRERkcVpUOEmPz8fVlaGJVtbWwMAhBBylNTg2NkBr7zy94u/XsXOKww3RERkWWQNN7m5uThx4gROnDgBALh27RpOnDiB69evA5C6lCZNmqTff/jw4fjll1+wZMkSXL16Ffv378dLL72E7t27w9/fX46v0CA9+yzg5FwCpLfC1s12DIZERGRRZA03R44cQefOndG5c2cAwCuvvILOnTtj3rx5AICkpCR90AGAKVOm4KOPPsLnn3+O9u3bY9y4cWjVqhV++eUXWepvqNRqYPp0BQBAs+t5nE09J3NFRERExqMQjex/27Ozs+Hi4gKNRtOox98kJwNNmhZDW2KHWUt/xsfPj5G7JCIioirdz+93gxpzQ8bj6ws8MPQsAGDt101lroaIiMh4GG4asVfnWAPQIuVYN5w6XSZ3OUREREbBcNOIjezdFjbtNgMA5i7MlLkaIiIi42C4acRsrGwQPmEfAGDLz554+mlAo5G5KCIiojpiuGnkxgwKAAbOBRRlWLUK6NQJ2L9f7qqIiIhqj+GmkRvYbCDQ5x3YPjMITZqWID4e6NsXmD8fKC2VuzoiIqL7x3DTyLXzaocBIQNQEhADr1cGIeKpMmi1wKJFQJ8+wJUrcldIRER0fxhuGjmFQoGVI1fCXeWOE1l/wO/p17BmDeDiAhw4IHVTrVgBNK67IRERUUPGcENo4twE3z3+HQDgg9gP4Nk9GidPSi03ubnA1KnA+PFAJi+oIiKiBoDhhgAAI1qPwIsPvAgAmLRxEhw8b2PPHuCttwAbG2D9eqBNG+A//wEuXpS5WCIiomow3JDeB4M/QFuvtkjOTcbUX6fCykrgP/8B/voLCA0FUlKAxYuBVq2ABx8EvvkGyM6Wu2oiIiJDDDek52DrgLVj1kJprcTvl37H54c+BwB06wacPi213jz6KGBlJV0u/o9/AH5+wKRJwJ49gFYr8xcgIiICH5wpdzn10ueHPsfMrTOhtFbi0D8OoaNPR4Ptt24BP/wALF8OxMXdWR8cLAWdUaOAsDBAoTBv3UREZLnu5/eb4YYqEELg8bWPY/PFzWjj2QZHnjsCB1uHSvYDDh6UQs7atYZdVAEBwGOPSdPAgYBDxcOJiIhqjOGmGgw3NXM77zbCloYhKTcJz3d9HkuHLa12//x8YONGKeTs3AkUFNzZZm8PDBgADBsmhZ2mfAg5ERHdJ4abajDc1NzOqzvx8A8PAwB+eeIXjGozqkbHFRQAMTHA778Dv/0GXL9uuL19e6BfP6BHD2kKDWUXFhERVY/hphoMN/fnX9H/wvt/vQ93lTtOvnASAc4B93W8EMDZs8DmzdIUG1tx4LG7O9C9uxR0evaUlt3djfgliIiowWO4qQbDzf0pLitG7+9648itI+gb1BebJmyCi71Lrd8vPV3qtjpwQBqvc+wYUFRUcb/Q0DstO927SwOUlco6fBEiImrQGG6qwXBz/y6lX0LnrzojryQP3o7eeGfgO5jcaTKsFHW/k0BxMXDypBR0Dh6UQs/lyxX3s7WVHgXRvfudqWVL6bJ0IiKyfAw31WC4qZ19Cfvwj9/+gbh06drv7k2647Ohn6F7k+5G/6z0dODQIcMpLa3ifs7OQJcuQIcOd6b27QEnJ6OXREREMmO4qQbDTe0VlxXjs4OfYeHehcgpzgEATOs0DW8PfBs+Tj4m+1whgPh4KeQcPCjNjx4FCgsr3z8kxDDwtGsHNG8OqFQmK5GIiEyM4aYaDDd1l5SThLm75uL7k98DAJyVzljYfyEiu0XC1trWLDWUlEgDlU+ckO6erJuSk6s+pkkToEULKejo5rpll9oPIyIiIjNguKkGw43xxCbGYubWmTiadBQA0NarLT555BMMajZItprS0gzDzunTwPnz934GlqenNIi5Zcs785YtpeDj6Gie2omIqGoMN9VguDGuMm0Zlp9Yjrm75iItXxoY09m3M57t8iye7PAkXO1d5S0QUrdWejpw5Yo0WPnueWpq9cc3aXIn8Ohae3STWm2e70BE1Ngx3FSD4cY0MgsyMT9mPr46+hWKy4oBACobFca1G4dnOz+LB5s+CEU9vVNfTo4Uci5dkqaLF+/M09OrP9bbu2LgCQoCAgMBf39evk5EZCwMN9VguDGt9Px0rDq1CsuOLcPZ22f161t5tMKzXZ7FpLBJ8Hb0lrHC+5ORcSfoXLwotfbopnsFHwDw8ZGesxUYKM11y0FBUhDy8eHdmYmIaoLhphoMN+YhhMChm4fwzbFv8OOZH5FXkgcAsLGywYhWIzCh/QQ80uIRONk13Ou2NRrDsKPr5kpMBG7cqPzmhHdzdASaNavY+tOihfQMLhsb038PIqKGgOGmGgw35pdTlIN1Z9fhm2Pf4ODNg/r1SmslBjUbhJGtR2J4y+EmvZzc3ISQBjffuHEn7JSfX7smzav7t8/KSuraCgy8MzVtavjay4s3MiSixoHhphoMN/I6nXIaK0+uRNSFKFzJvKJfr4AC4YHhGNlqJEa2HolQj1AZqzSPoiLp/j13t/5cuQJcvVqzlh87O8DPTwpB1c09PNj9RUQNG8NNNRhu6gchBM7dPodf437FxgsbcfjWYYPtbTzb4NHQRzGo2SD0adoHjnaN63psrVa6Z09i4p3p+nXD18nJ1bf8lGdjI43v8fWtONct+/lJE68AI6L6iOGmGgw39dON7BvYFLcJGy9sxJ74PSjVluq32VrZomdATwxqNggDQwaie5PuZrtZYH1WXAzcugUkJVU/r8nA5/KcnO60+tzdAqQLQj4+0pPb2SVGRObCcFMNhpv6L6swC9sub8POqzux8+pOJGgSDLY72TmhX1A/DAwZiP7B/dHBpwNsrDjytipFRdK9fFJSpNaeu+flp5ycmr+vjY005kcXdnx8pEvjvbyk1h9nZ2l+9+TsLA2kZjcZEd0PhptqMNw0LEIIXM28il3XdmHn1Z3YfW030gsMmyIcbR3RrUk39GzSE+GB4egZ0LNBXW5en+TkSC0+VbUC6UJRZmbdPkehkAKOLvA4OVWcOztLN1AMDr4zubkxFBE1Vgw31WC4adi0QotTKaew6+ou7Ly2E7GJsdAUaSrs18ytGXoG9ETPJj3RM6AnOvh0gL2NvQwVW6biYuD2bSno3D2lp0shqfyUnX1nWaut/eeq1YZhJyhI6i7z9JQmDw9p7uDAEERkaRhuqsFwY1m0QosLaRcQmxiLAzcOIPZGLM7dPgcBw3+srRXWaOfdDp19O6OLXxd08euCMJ8wqJUcPWtOQgAFBVLIyc29E3gqW9ZopIHT8fHSlJJS88+xtzcMO25ugKur9IDUqqby23l/IaL6h+GmGgw3lk9TqMGhm4f0YefwrcP6516Vp4ACoR6h6OLXBZ19O6O9d3u082qHpi5N6+2jIhqzggLpijFd2ImPBxISpNCTlia1GN2+LbUq1ZWj452w4+pquKxrJfL0lMYXlX+tUtX9s4mocgw31WC4aXyEELiZcxPHko7hWNIxHE8+jmNJx3Aj+0al+6vt1Gjr1RbtvNpJgce7Hdp5tYO/2p+hp54TAsjLk8KObrp9WxojpNFUP2VlSQGqLhwcpMCjaynSzcsvl19XvuVIrebVZ0TVYbipBsMN6aTmpeJ40nEcT5ams6lnEZceZ3AZenmu9q5o7dlamjxa65ebuTXjpekWoqTkTtApP9ctZ2ZKLUS60FQ+RJWU1O2zFQppEHX5bjInJ6mLzd5eahXSLZd/7eho2Irk5SV1x7FrjSwNw001GG6oOsVlxbiUfglnb5/F2dSzOHP7DM6mnsWljEvQispHwtpY2aC5W3N92Gnp0RKh7qEI9QiFj6MPW3saASGkQdO6oJOZeScMVTUvH5rqGowq4+ZmGHbKX5pf1WX6trZS65G1ddWTLlCpVNJrInNhuKkGww3VRmFpIeLS4hCXHocLaRdwIe2Cfjm/JL/K49R2arRwb4FQj1Ap8Pwdelq4t4CXgxeDD0EIoLCwYheZRiN1sRUWSt1lhYV3pvKvc3IMW5IyMmp+5+q60gUdBwfDuZPTncv6y1/iX/61o6MUpmxtpceI6JbLT0qltL+9Pa9+I4abajHckDFphRY3s2/qA8+FtAu4lHEJlzIuISErocJVW+U52joixC0EIa4haObW7M7873WN7ZETZBxlZVLAubvr7O7L8+++TD83V2pBKiuTJq32zrJuKi2t2TPPjM3G5k5rk7Oz4aRWS6Gquq47e3tpP91YJ914J7Y8NSwMN9VguCFzKSotwtXMq1LYSb+kDz2X0i/hRvaNaoMPAHg7eiPYNRhBLkEIdg02WA5yDYKTnZOZvgnRHbrL+fPygPx8aX73cl6eYYCq7FL/vDwpTFU3GePKt+rcHXhUKinUVTcpFFKY0k26Fqu7J6VSapGys6t6WauVvmdpqTTplsuvc3CQHnXi4SHN3d2lYxsjhptqMNxQfVBUWoQETQKuZl7Ftcxr0jzrGq5lSctZhVn3fA8PlQeCXYPR1KUpmro0RaBzoH65qUtT+Dj5wErBy2+o4dJqpRCUnV35pGt50nXTVdV9p7u3UlaWNOVX3ZPcIDg5GQYeF5eKrVXlW610y0ql4VTZuvLjq6oaf6VUytNNyHBTDYYbaggyCzJxLesaErISkKBJQHxWPOKz4vXLNQk/tla2CHAOkIKPS6A+/AQ6B+pfu9q7ctwPNTrFxXfGNpWfCgru/KhXNWm1hi1XVU1FRdLnFBffWb57nbW1NLbIxubO/O7l/HzpCr30dGkgen34xbaxMbzxZWVzLy9g+nTjfi7DTTUYbsgSZBVm6YNPoiYR1zXXcT37ujTXXMetnFtVXt1VnqOtoz7oBDgHoIm6CZo4N9HP/dX+8Hb0ZgsQUT2g1UohLCNDCju6eU5OxZarypaLiu5Md7/WTeXHWNXlUSk+PtKz6IyJ4aYaDDfUGJRqS3Er55Y+7CRqEpGY/ff093Jld22ujI2VDfyc/PShx8/JD/5qf/ir/eGnvrPsZu/GViAiC3P3wPLSUmnc1N33grp7rlIBH31k3FoYbqrBcEMkyS/Jx43sG/qwczP7Jm7m/D39vZySm3LPgc86Smsl/NR+8HPyg6+Tr35+9+Tj5AM760Y6IpKIao3hphoMN0Q1V1JWguTcZH3gScpNwq2cW7iVc8tgOaMg477e10PlUSH03B2G/NR+cLV3ZZcYEQFguKkWww2R8RWWFiI5N1kKPTlJSMlLQVJOEpJzk5GclyzN/56qerxFZawV1vB08ISXoxe8HLzg5egFbwdvw9eO3vB29IaPow8HSBNZsPv5/ebTR4iozuxt7PX34qmOVmiRWZCJpNwkfdgpH4L0y7nJyCzMRJkoQ0peClLyUmpUh62VrT7seDt6w8fJRx+GPFQe8HTwhIfD33OVB9xUbrCx4n8GiSwNW26IqF4qKi3C7fzbuJ13u/J5/m2k5qXidt5tpOSlILsou1af42bvBg8HD8PwozIMQbr1HioPuKvcobRRGvnbEtG9NJiWmz/++APvv/8+jh49iqSkJERFRWHkyJHVHlNUVIRFixZh1apVSE5Ohp+fH+bNm4dp06aZp2giMguljRIBzgEIcA6o0f6FpYX6oJOal4qU3L/neSlIy09DekG6NM9PR3pBuv5eQZmFmcgszMRlXK5xbQ62DnCzd4O7yh3uKne4qdzgbn9n2c3erdK5q70rW4qIzEDWf8vy8vIQFhaGadOmYfTo0TU65oknnkBKSgq+/fZbtGjRAklJSdDW5WJ8IrII9jb20j17XAJrtH+pthQZBRlIz0/Xh5/yy3cHorT8NGQUZEBAIL8kH/kl+biZc/O+63RWOsPN3g0u9i5wUbro585KZ4PXurmrvavB5GDrwHFFRPcga7gZOnQohg4dWuP9t23bhr179+Lq1atwd3cHAAQHB5uoOiKyZDZWNvqxOTWlFVpkF2UjoyADmQWZyCjIkJYLMw3W6VqDyr/OLc4FAGQXZUtdaJra110+7BgEokrCkYv9neCkVqrhrHSGo60jAxJZtAbVPrpp0yY88MADeO+99/DDDz/A0dERjz/+ON58802oVKpKjykqKkJRucfYZmfXrl+eiMhKYaUPFXC7v2NLykqQVZilDz2aIg00hRpoijTILsrWL9/9OqswSz+VaktRqi1FWn5ajW/CWNX3UNtJQUcXePSTnbPB67u3q+3UcLJzglopzVU2KgYlqncaVLi5evUq/vzzT9jb2yMqKgppaWmYPn060tPTsXz58kqPWbx4MRYuXGjmSomIDNla20qXsDt61ep4IaTusPJhRzeVD0rlQ1L5dTnFOcguyoZWaKEVWv0+daWAwiDsONk5GQQgtd3fk9Jw7mTnBEc7RzjaOlaYs+uN6qreXC2lUCjuOaB48ODB2LdvH5KTk+Hi4gIA+OWXXzB27Fjk5eVV2npTWctNYGAgr5YiokZHF5B0XWO6wKNrKcopzkFO0Z112cXZd5Z1xxTlILc4F3kleSat1cHWQR94VDYqONg6VDlVFarKhy4HWweobFSwt7GHjZUNw1MD1GCulrpffn5+aNKkiT7YAECbNm0ghMCNGzcQGhpa4RilUgmlkpdtEhEpFAqpdcTOEX5qvzq9l1ZokV+Srw87ucW5+nB09+vy8/Lr80rykFecp58XlBbo3183aPt2/u26fu0KrBRWsLexrzCpbFQVW5MqaVlS2ar0+6tsVfrQVH65/GRtZW3070DVa1Dhpnfv3li/fj1yc3Ph5OQEALh48SKsrKwQEFCzy0WJiKjurBRW+hYTY9EFpvKBJ78kHwWlBfqwc/ek27d8sNKHqHLBqqisqMLn5JfkG6326lgrrA3CjtJGaRCOdK1KKlsVHGwc9CHJwdbhTvCyrRia7g5muuN088YcqmQNN7m5ubh8+c69Ja5du4YTJ07A3d0dTZs2xdy5c3Hz5k2sXLkSAPDkk0/izTffxNSpU7Fw4UKkpaXh1VdfxbRp06ocUExERA2DKQKTjlZoUVxWjMLSwkqnghIpQN3dmmQw/3u5sLQQBaUFKCgpqHK5TJTpP7tMlOmPNyc7azuDsGNnbQeljVKaWysrfW1vYw+ltVIfwqpbrm6uslHVuXWwLmQNN0eOHMFDDz2kf/3KK68AACZPnowVK1YgKSkJ169f1293cnJCdHQ0Zs6ciQceeAAeHh544okn8L///c/stRMRUcNRvivKHEq1pSgqLUJhaSGKyor0IUq3rqC0wCBU6VqnCkoKDJYLSwtRWFZ5INNN5Y8rLivW11BcVozismKjDBy/X96O3kiZU7PHpphCvRlQbC58/AIREVmqMm2ZQQtS+XlxWTGKSoukeVlRhddFpUX6uS6UFZUWobCsUL9NF9Du3v/uubejN67OumrU72axA4qJiIioatZW1vpB442ZldwFEBERERkTww0RERFZFIYbIiIisigMN0RERGRRGG6IiIjIojDcEBERkUVhuCEiIiKLwnBDREREFoXhhoiIiCwKww0RERFZFIYbIiIisigMN0RERGRRGG6IiIjIojDcEBERkUWxkbsAcxNCAACys7NlroSIiIhqSve7rfsdr06jCzc5OTkAgMDAQJkrISIiovuVk5MDFxeXavdRiJpEIAui1Wpx69YtqNVqKBSKGh+XnZ2NwMBAJCYmwtnZ2YQVEsDzbW483+bF821ePN/mZarzLYRATk4O/P39YWVV/aiaRtdyY2VlhYCAgFof7+zszH85zIjn27x4vs2L59u8eL7NyxTn+14tNjocUExEREQWheGGiIiILArDTQ0plUrMnz8fSqVS7lIaBZ5v8+L5Ni+eb/Pi+Tav+nC+G92AYiIiIrJsbLkhIiIii8JwQ0RERBaF4YaIiIgsCsMNERERWRSGmxr44osvEBwcDHt7e/To0QOHDh2SuySL8ccff2D48OHw9/eHQqHAxo0bDbYLITBv3jz4+flBpVJh0KBBuHTpkjzFNnCLFy9Gt27doFar4e3tjZEjRyIuLs5gn8LCQkRGRsLDwwNOTk4YM2YMUlJSZKq4YVuyZAk6duyov5FZeHg4tm7dqt/Oc21a77zzDhQKBWbPnq1fx3NuPAsWLIBCoTCYWrdurd8u97lmuLmHdevW4ZVXXsH8+fNx7NgxhIWFYciQIUhNTZW7NIuQl5eHsLAwfPHFF5Vuf++99/Dpp59i6dKlOHjwIBwdHTFkyBAUFhaaudKGb+/evYiMjMSBAwcQHR2NkpISDB48GHl5efp9Xn75Zfz2229Yv3499u7di1u3bmH06NEyVt1wBQQE4J133sHRo0dx5MgRDBgwACNGjMDZs2cB8Fyb0uHDh/HVV1+hY8eOBut5zo2rXbt2SEpK0k9//vmnfpvs51pQtbp37y4iIyP1r8vKyoS/v79YvHixjFVZJgAiKipK/1qr1QpfX1/x/vvv69dlZWUJpVIpfvzxRxkqtCypqakCgNi7d68QQjq3tra2Yv369fp9zp8/LwCI2NhYucq0KG5ubuKbb77huTahnJwcERoaKqKjo0W/fv3ErFmzhBD859vY5s+fL8LCwirdVh/ONVtuqlFcXIyjR49i0KBB+nVWVlYYNGgQYmNjZayscbh27RqSk5MNzr+Liwt69OjB828EGo0GAODu7g4AOHr0KEpKSgzOd+vWrdG0aVOe7zoqKyvD2rVrkZeXh/DwcJ5rE4qMjMRjjz1mcG4B/vNtCpcuXYK/vz+aNWuGiIgIXL9+HUD9ONeN7sGZ9yMtLQ1lZWXw8fExWO/j44MLFy7IVFXjkZycDACVnn/dNqodrVaL2bNno3fv3mjfvj0A6Xzb2dnB1dXVYF+e79o7ffo0wsPDUVhYCCcnJ0RFRaFt27Y4ceIEz7UJrF27FseOHcPhw4crbOM/38bVo0cPrFixAq1atUJSUhIWLlyIPn364MyZM/XiXDPcEDVCkZGROHPmjEEfORlfq1atcOLECWg0GmzYsAGTJ0/G3r175S7LIiUmJmLWrFmIjo6Gvb293OVYvKFDh+qXO3bsiB49eiAoKAg//fQTVCqVjJVJ2C1VDU9PT1hbW1cY4Z2SkgJfX1+Zqmo8dOeY59+4ZsyYgc2bN2PPnj0ICAjQr/f19UVxcTGysrIM9uf5rj07Ozu0aNECXbt2xeLFixEWFoZPPvmE59oEjh49itTUVHTp0gU2NjawsbHB3r178emnn8LGxgY+Pj485ybk6uqKli1b4vLly/Xin2+Gm2rY2dmha9eu2LVrl36dVqvFrl27EB4eLmNljUNISAh8fX0Nzn92djYOHjzI818LQgjMmDEDUVFR2L17N0JCQgy2d+3aFba2tgbnOy4uDtevX+f5NhKtVouioiKeaxMYOHAgTp8+jRMnTuinBx54ABEREfplnnPTyc3NxZUrV+Dn51c//vk2y7DlBmzt2rVCqVSKFStWiHPnzonnnntOuLq6iuTkZLlLswg5OTni+PHj4vjx4wKA+Oijj8Tx48dFQkKCEEKId955R7i6uopff/1VnDp1SowYMUKEhISIgoICmStveF588UXh4uIiYmJiRFJSkn7Kz8/X7/PCCy+Ipk2bit27d4sjR46I8PBwER4eLmPVDddrr70m9u7dK65duyZOnTolXnvtNaFQKMSOHTuEEDzX5lD+aikheM6N6Z///KeIiYkR165dE/v37xeDBg0Snp6eIjU1VQgh/7lmuKmBzz77TDRt2lTY2dmJ7t27iwMHDshdksXYs2ePAFBhmjx5shBCuhz8jTfeED4+PkKpVIqBAweKuLg4eYtuoCo7zwDE8uXL9fsUFBSI6dOnCzc3N+Hg4CBGjRolkpKS5Cu6AZs2bZoICgoSdnZ2wsvLSwwcOFAfbITguTaHu8MNz7nxjB8/Xvj5+Qk7OzvRpEkTMX78eHH58mX9drnPtUIIIczTRkRERERkehxzQ0RERBaF4YaIiIgsCsMNERERWRSGGyIiIrIoDDdERERkURhuiIiIyKIw3BAREZFFYbghokZJoVBg48aNcpdBRCbAcENEZjdlyhQoFIoK0yOPPCJ3aURkAWzkLoCIGqdHHnkEy5cvN1inVCplqoaILAlbbohIFkqlEr6+vgaTm5sbAKnLaMmSJRg6dChUKhWaNWuGDRs2GBx/+vRpDBgwACqVCh4eHnjuueeQm5trsM93332Hdu3aQalUws/PDzNmzDDYnpaWhlGjRsHBwQGhoaHYtGmTfltmZiYiIiLg5eUFlUqF0NDQCmGMiOonhhsiqpfeeOMNjBkzBidPnkRERAQmTJiA8+fPAwDy8vIwZMgQuLm54fDhw1i/fj127txpEF6WLFmCyMhIPPfcczh9+jQ2bdqEFi1aGHzGwoUL8cQTT+DUqVN49NFHERERgYyMDP3nnzt3Dlu3bsX58+exZMkSeHp6mu8EEFHtme0RnUREf5s8ebKwtrYWjo6OBtNbb70lhJCeYP7CCy8YHNOjRw/x4osvCiGE+Prrr4Wbm5vIzc3Vb//999+FlZWVSE5OFkII4e/vL15//fUqawAg/vvf/+pf5+bmCgBi69atQgghhg8fLqZOnWqcL0xEZsUxN0Qki4ceeghLliwxWOfu7q5fDg8PN9gWHh6OEydOAADOnz+PsLAwODo66rf37t0bWq0WcXFxUCgUuHXrFgYOHFhtDR07dtQvOzo6wtnZGampqQCAF198EWPGjMGxY8cwePBgjBw5Er169arVdyUi82K4ISJZODo6VugmMhaVSlWj/WxtbQ1eKxQKaLVaAMDQoUORkJCALVu2IDo6GgMHDkRkZCQ++OADo9dLRMbFMTdEVC8dOHCgwus2bdoAANq0aYOTJ08iLy9Pv33//v2wsrJCq1atoFarERwcjF27dtWpBi8vL0yePBmrVq3Cxx9/jK+//rpO70dE5sGWGyKSRVFREZKTkw3W2djY6Aftrl+/Hg888AAefPBBrF69GocOHcK3334LAIiIiMD8+fMxefJkLFiwALdv38bMmTPx9NNPw8fHBwCwYMECvPDCC/D29sbQoUORk5OD/fv3Y+bMmTWqb968eejatSvatWuHoqIibN68WR+uiKh+Y7ghIlls27YNfn5+ButatWqFCxcuAJCuZFq7di2mT58OPz8//Pjjj2jbti0AwMHBAdu3b8esWbPQrVs3ODg4YMyYMfjoo4/07zV58mQUFhbi//7v/zBnzhx4enpi7NixNa7Pzs4Oc+fORXx8PFQqFfr06YO1a9ca4ZsTkakphBBC7iKIiMpTKBSIiorCyJEj5S6FiBogjrkhIiIii8JwQ0RERBaFY26IqN5hbzkR1QVbboiIiMiiMNwQERGRRWG4ISIiIovCcENEREQWheGGiIiILArDDREREVkUhhsiIiKyKAw3REREZFEYboiIiMii/D/mIZDpTbjo/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = list(range(1,num_epochs+1))\n",
    "\n",
    "\n",
    "plt.plot(x_axis,list_of_train_loss,color=\"green\",label=\"train_loss\")\n",
    "plt.plot(x_axis,list_of_val_loss,color=\"blue\",label=\"val_loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Validation loss plot\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78c2da",
   "metadata": {
    "papermill": {
     "duration": 0.054308,
     "end_time": "2023-05-04T09:53:04.661530",
     "exception": false,
     "start_time": "2023-05-04T09:53:04.607222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Woah! our simple model works quite good as we can see both the train and the val loss decreases steadily towards the start of the  epoch and with the increase in the epoch number the decrease is less steeper indicating that the model is tending towards convergence.\n",
    "\n",
    "It might have helped to train it for further epochs, we could have also checked for convergence by setting a threshold.Where when the decrease in loss is less than that of the threshold, we can consider convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2563b1",
   "metadata": {
    "papermill": {
     "duration": 0.052923,
     "end_time": "2023-05-04T09:53:04.768280",
     "exception": false,
     "start_time": "2023-05-04T09:53:04.715357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8a5fd",
   "metadata": {
    "papermill": {
     "duration": 0.053003,
     "end_time": "2023-05-04T09:53:04.874085",
     "exception": false,
     "start_time": "2023-05-04T09:53:04.821082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3912.553774,
   "end_time": "2023-05-04T09:53:08.685620",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-04T08:47:56.131846",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2447d64026ee4d9b8e78ba7223093f11": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28d53aca6ba44a62bb5d1c24ea0fa46e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f46ee41dae4b41ff94744a6f974d0193",
       "placeholder": "​",
       "style": "IPY_MODEL_57de7b6666824d5891a0e98fa507195a",
       "value": "100%"
      }
     },
     "2acf068e5beb41b8bf287f6256311688": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2e246a98ab824fc1bbc6a1d46534f1fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2447d64026ee4d9b8e78ba7223093f11",
       "max": 50.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_33a2d90e66824f75b6d0b24ec4abf32d",
       "value": 50.0
      }
     },
     "33a2d90e66824f75b6d0b24ec4abf32d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "57de7b6666824d5891a0e98fa507195a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6bc1b445a2bb4a5b8abdcbddc85eec04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9ae77b459a774b75813ca7c824c1bc66": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c9cf6a3bbc1140b6843835fb88aabeee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2acf068e5beb41b8bf287f6256311688",
       "placeholder": "​",
       "style": "IPY_MODEL_6bc1b445a2bb4a5b8abdcbddc85eec04",
       "value": " 50/50 [1:03:51&lt;00:00, 76.63s/it]"
      }
     },
     "de7653cb031a4330bf8517c1f967fa58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_28d53aca6ba44a62bb5d1c24ea0fa46e",
        "IPY_MODEL_2e246a98ab824fc1bbc6a1d46534f1fe",
        "IPY_MODEL_c9cf6a3bbc1140b6843835fb88aabeee"
       ],
       "layout": "IPY_MODEL_9ae77b459a774b75813ca7c824c1bc66"
      }
     },
     "f46ee41dae4b41ff94744a6f974d0193": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
